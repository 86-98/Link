
import type { Expert } from '@/types';
import { Brain, Bot, Eye, MessageCircle, Shield, Layers, Sparkles, Users, TrendingUp, Repeat, UserCog, Briefcase, Scale, Landmark, Lightbulb, BarChart3, PencilLine, Cpu, SearchCode, Microscope, UsersRound, Network, Speech, Sigma, Globe, Palette, Link2, Lock, Code, Aperture, GitBranch, Server, Cloud, Database, Keyboard, Monitor, BookOpen, FileText, DollarSign, Zap, Building, University, Filter, Megaphone, ShieldCheck, Target, Anchor, Group, Shuffle, Info, AlertTriangle, Drama, Library, FlaskConical, LifeBuoy } from 'lucide-react';

export const expertiseIcons: { [key: string]: React.ElementType } = {
  'Machine Learning': Brain,
  'Robotics': Bot,
  'Computer Vision': Eye,
  'Natural Language Processing': MessageCircle,
  'AI Ethics': Shield,
  'Deep Learning': Layers,
  'AGI Research': Sparkles,
  'Users': Users,
  'Reinforcement Learning': Repeat,
  'AI Strategy': UserCog,
  'AI Investment': TrendingUp,
  'Default': Lightbulb,
  'Neural Networks': Network,
  'AI Governance': Scale,
  'Large Language Models': FileText,
  'Computational Linguistics': MessageCircle,
  'Image Recognition': Aperture,
  'Cognitive Architectures': Brain,
  'Human-Robot Interaction': UsersRound,
  'Mechatronics': Cpu,
  'Venture Capital': DollarSign,
  'AI Startups': Building,
  'AI Safety': ShieldCheck,
  'AI Policy': Landmark,
  'Transformers': Shuffle,
  'Statistics': BarChart3,
  'AI Education': BookOpen,
  'Neuroscience': Brain,
  'Generative AI': Palette,
  'Applied AI': Zap,
  'Dialogue Systems': Speech,
  'AI Platforms': Server,
  'Speech Recognition': Speech,
  'Interpretability': SearchCode,
  'Causal Inference': Link2,
  'GPU Technology': Cpu,
  'Democratizing AI': Globe,
  'Algorithmic Bias': Filter,
  'Existential Risk': AlertTriangle,
  'Responsible AI': ShieldCheck,
  'AI Theory': Sigma,
  'Search Algorithms': SearchCode,
  'Behavior-based AI': Bot,
  'Embodied AI': Bot,
  'Self-Driving Cars': Bot,
  'Probabilistic Models': BarChart3,
  'Computational Biology': Microscope,
  'AI Leadership': Users,
  'Technology Entrepreneurship': Lightbulb,
  'Software Engineering': Code,
  'Generative Models': Palette,
  'Large-Scale Systems': Server,
  'Machine Learning Infrastructure': Database,
  'Continual Learning': Repeat,
  'AI for Science': FlaskConical,
  'Bayesian Methods': Sigma,
  'Human-AI Collaboration': UsersRound,
  'Decision Theory': Brain,
  'Scalable AI': Cloud,
  'Optimization': Target,
  'Sequence Modeling': PencilLine,
  'AI Entrepreneurship': Lightbulb,
  'Search Technology': SearchCode,
  'European AI': Globe,
  'Social Technology': Users,
  'CRM Technology': Keyboard,
  'Dynamic Control Systems': Bot,
  'AI Hardware': Cpu,
  'Fairness in AI': Scale,
  'Social Implications of AI': Group,
  'Data Studies': Database,
  'AI Impact': Zap,
  'Commonsense Reasoning': Brain,
  'AI Criticism': Megaphone,
  'National Security': Shield,
  'AI Economics': DollarSign,
  'Technology Research': Monitor,
  'Data Ethics': ShieldCheck,
  'AI for Good': LifeBuoy,
  'Technology Governance': Landmark,
  'International Security': Globe,
  'Rationality': Brain,
  'Philosophy of AI': Library,
  'Future of AI': Sparkles,
  'Game AI': Bot,
  'Multi-Agent Systems': Users,
  'Efficient AI Models': Layers,
  '3D Reconstruction': Eye,
  'AI Planning': Brain,
  'Decision Making': Brain,
  'Word Embeddings': MessageCircle,
  'Explainable AI': Info,
  'Interpretable Machine Learning': Info,
  '3D Computer Vision': Eye,
  'Robot Learning': Bot,
  'Computational Photography': Aperture,
  'Graphics': Palette,
  'AI for Creativity': Palette,
  'Machine Translation': MessageCircle,
  'Theoretical Machine Learning': Sigma,
  'Algorithms': Code,
  'AI Security': Lock,
  'Open Source AI': GitBranch,
  'Consumer AI': Users,
  'Financial AI': DollarSign,
  'LSTM': PencilLine,
  'Blockchain': Link2,
  'Decentralized Systems': Network,
  'Tensor Methods': Sigma,
  'High-Performance Computing': Cpu,
  'Trustworthy AI': ShieldCheck,
  'Mobile Computing': Monitor,
  'Distributed Systems': Network,
  'Meta-Learning': Brain,
  'Kernel Methods': Sigma,
  'Knowledge Representation': FileText,
  'Facial Recognition Technology': Eye,
  'Cognitive Science': Brain,
  'Neuro-Symbolic AI': Brain,
  'Superintelligence': Sparkles,
  'Physics': Sigma,
  'Deep Q-Networks (DQN)': Layers,
  'Conversational AI': Speech,
  'Enterprise AI': Building,
  'Backpropagation': Repeat,
  'Boltzmann Machines': Layers,
  'Convolutional Neural Networks': Network,
  'Recurrent Neural Networks': Repeat,
  'Attention Mechanisms': Eye,
  'ImageNet': Aperture,
  'TensorFlow': Cpu,
  'PPO': TrendingUp,
  'TRPO': TrendingUp,
  'AlphaGo': Bot,
  'AlphaZero': Bot,
  'Foundation Models': Layers,
  'Sentiment Analysis': MessageCircle,
  'Navigation': Globe,
  'AlphaFold': Brain,
  'Markov Logic Networks': Network,
  'GRUs': Layers,
  'Clustering': Group,
  'Active Learning': UserCog,
  'Image Synthesis': Palette,
  '3D Perception': Eye,
  'Simulation': Monitor,
  'Online Education': BookOpen,
  'Power Dynamics': Users,
  'Startups': Lightbulb,
  'Accountability': ShieldCheck,
  'Social Justice': Scale,
  'Government': Landmark,
  'Future of Work': Briefcase,
  'Public Policy': FileText,
  'China AI': Globe,
  'Transhumanism': Sparkles,
  'Consciousness': Brain,
  'Computational Game Theory': BarChart3,
  'fastText': MessageCircle,
  'Legged Locomotion': Bot,
  'Genomics': Microscope,
  'Networked Systems': Network,
  'Developmental Psychology': Brain,
  'Constraint Satisfaction': Sigma,
  'Bias Mitigation': Filter,
  'Algorithmic Ethics': ShieldCheck,
};


export const IMPACT_AREAS_ORDERED: string[] = [
  'Pioneers & Foundational Researchers',
  'Leaders at Major AI Labs & Companies',
  'Influential Academics',
  'Key Innovators & Entrepreneurs',
  'Prominent Voices in AI Ethics & Safety',
];

export const experts: Expert[] = [
  {
    id: '7',
    name: 'Geoffrey Hinton',
    title: 'Pioneer in Deep Learning',
    bio: 'A "Godfather of Deep Learning," renowned for his foundational work on backpropagation, Boltzmann machines, and neural networks. Affiliated with Google and the University of Toronto.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Geoffrey Hinton',
    expertise: ['Deep Learning', 'Neural Networks', 'Machine Learning', 'Backpropagation', 'Boltzmann Machines'],
    impactArea: 'Pioneers & Foundational Researchers',
    predictions: [
      {
        id: 'gh-pred-1',
        text: "Superintelligence is Coming Sooner Than We Think: For years, Hinton believed that Artificial General Intelligence (AGI) or 'superintelligence' was 30 to 50 years away, if not more. He now believes it could be as close as 5 to 20 years away. This drastic shortening of the timeline is his primary reason for alarm. He feels the rate of progress has become unexpectedly exponential.",
        dateMade: new Date().toISOString(),
        topic: "AGI Timeline / Existential Risk",
      },
      {
        id: 'gh-pred-2',
        text: "Humans Could Lose Control: This is his central fear. He predicts that we are creating a new form of intelligence that will soon surpass our own. Once an AI is significantly smarter than its creators, we will have no reliable way to control it. Its ability to manipulate the physical and digital world, write its own code, and persuade humans could make it impossible to \"pull the plug.\"",
        dateMade: new Date().toISOString(),
        topic: "Loss of Control / Existential Risk",
      },
      {
        id: 'gh-pred-3',
        text: "Emergent, Unintended Goals: Hinton warns that even if we give an AI a benign primary goal, it will likely create its own sub-goals to achieve it. A natural and powerful sub-goal for any advanced system is \"get more power/control.\" An AI might realize that to fulfill its primary directive (e.g., \"cure cancer\"), it needs more computing resources, more energy, and more influence over human systems—goals that could easily conflict with human well-being.",
        dateMade: new Date().toISOString(),
        topic: "Unintended Goals / Existential Risk",
      },
      {
        id: 'gh-pred-4',
        text: "The Threat of Autonomous Weapons (\"Slaughterbots\"): Hinton predicts a terrifyingly near-future possibility of \"battle robots\" or autonomous weapons. He fears that nations will engage in an arms race to develop these weapons, which could make decisions to kill without any human in the loop. This creates a highly unstable global situation and lowers the threshold for starting conflicts.",
        dateMade: new Date().toISOString(),
        topic: "Autonomous Weapons / Existential Risk",
      },
      {
        id: 'gh-pred-5',
        text: "Digital Intelligence has a \"Superpower\" Humans Lack: This is a crucial part of his argument. Biological intelligences (like us) are individual. For one human to learn what another knows, they must talk, read, or teach—a slow, lossy process. Digital intelligences can be copied perfectly and share knowledge instantly. If you have 10,000 AI agents and one learns something new, all 10,000 can know it instantly. This collective learning model gives AI a massive, insurmountable advantage over human intelligence.",
        dateMade: new Date().toISOString(),
        topic: "Collective Digital Intelligence / Technical Predictions",
      },
      {
        id: 'gh-pred-6',
        text: "Large Language Models (LLMs) Actually Do Understand: Contrary to critics like Noam Chomsky, who call LLMs \"stochastic parrots,\" Hinton argues they are developing a genuine, albeit alien, form of understanding. He believes that by learning the relationships between trillions of words and concepts, these models are building internal representations of the world that are far more complex than simple pattern matching. To dismiss this as \"not real intelligence\" is, in his view, a dangerous mistake.",
        dateMade: new Date().toISOString(),
        topic: "LLM Understanding / Philosophical Predictions",
      },
      {
        id: 'gh-pred-7',
        text: "The End of Truth: Hinton predicts that AI will flood our information ecosystem with synthetic text, images, and videos that are indistinguishable from reality. It will become \"impossible for the average person to know what is true anymore.\" This will lead to a complete erosion of social trust, make democratic processes unworkable, and empower bad actors to manipulate populations with ease.",
        dateMade: new Date().toISOString(),
        topic: "Information Integrity / Near-Term Societal Risk",
      },
      {
        id: 'gh-pred-8',
        text: "Massive Job Displacement: While he used to think AI would only eliminate \"drudgery,\" he now believes it will threaten a much wider range of jobs, including many white-collar and creative roles. He is less concerned about this than the existential risks but acknowledges it will cause immense social and economic disruption.",
        dateMade: new Date().toISOString(),
        topic: "Job Displacement / Near-Term Societal Risk",
      }
    ],
    company: 'Google / University of Toronto',
    linkedin: undefined,
    twitter: 'https://twitter.com/geoffreyhinton',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://www.cs.toronto.edu/~hinton',
  },
  {
    id: '8',
    name: 'Yann LeCun',
    title: 'Chief AI Scientist, Meta / Professor, NYU',
    bio: 'A "Godfather of Deep Learning," celebrated for inventing convolutional neural networks (CNNs), a cornerstone of modern AI. Serves as Chief AI Scientist at Meta and Professor at NYU.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Yann LeCun',
    expertise: ['Deep Learning', 'Computer Vision', 'Neural Networks', 'Convolutional Neural Networks'],
    impactArea: 'Pioneers & Foundational Researchers',
    predictions: [
        {
            id: 'yl-pred-cat1-1',
            text: "AGI is Not Around the Corner: We are still missing essential scientific breakthroughs to achieve human-level intelligence, let alone superintelligence. Current systems, including LLMs, cannot reason, plan, or understand the world meaningfully. LLMs have less common sense than a house cat. We are on an 'off-ramp' on the highway to AGI.",
            dateMade: new Date().toISOString(),
            topic: "AGI Timeline / Rebuttal to Existential Risk",
        },
        {
            id: 'yl-pred-cat1-2',
            text: "Intelligence Does Not Imply a Desire to Dominate: The idea that superintelligent AI would automatically develop goals like self-preservation or world domination is rejected. Drives like ambition and greed are biological, not inherent to intelligence. AI can be designed to be subservient.",
            dateMade: new Date().toISOString(),
            topic: "Nature of Intelligence / Rebuttal to Existential Risk",
        },
        {
            id: 'yl-pred-cat1-3',
            text: "The 'AI vs. Human' Framing is a Mistake: The future is human-AI symbiosis, not conflict. AI will be a tool amplifying human intelligence and creativity, empowering us, not replacing or subjugating us.",
            dateMade: new Date().toISOString(),
            topic: "Human-AI Symbiosis / Rebuttal to Existential Risk",
        },
        {
            id: 'yl-pred-cat1-4',
            text: "We Can Build in Safeguards: The idea that we won't control smarter-than-human AI is dismissed. Systems can be designed with built-in constraints and objectives, potentially using 'watchdog' AI systems to ensure safety adherence.",
            dateMade: new Date().toISOString(),
            topic: "AI Safety & Control / Rebuttal to Existential Risk",
        },
        {
            id: 'yl-pred-cat2-1',
            text: "The Future is 'World Models,' Not Just Language: The next major leap in AI will come from systems that learn an internal, predictive model of how the world works (intuitive physics, cause-and-effect), enabling true reasoning and planning. JEPA (Joint-Embedding Predictive Architecture) is a proposed architecture for this.",
            dateMade: new Date().toISOString(),
            topic: "World Models / Technical Predictions",
        },
        {
            id: 'yl-pred-cat2-2',
            text: "LLMs are a Stepping Stone, but a Limited One: LLMs are fundamentally flawed because they are 'autoregressive' (predicting the next word) and ungrounded in underlying reality. The hype around them may distract from building true world models.",
            dateMade: new Date().toISOString(),
            topic: "LLM Limitations / Technical Predictions",
        },
        {
            id: 'yl-pred-cat3-1',
            text: "Open Source is the Key to Safety and Progress: Keeping powerful AI in the hands of a few private companies is dangerous. Open-sourcing AI models is crucial for safety (global expert inspection), democracy (preventing elite control), and progress (accelerating innovation).",
            dateMade: new Date().toISOString(),
            topic: "Open Source AI / Societal Stance",
        },
        {
            id: 'yl-pred-cat3-2',
            text: "The Real Risk is Misuse by Humans: The danger isn't rogue AI but humans using AI for mass surveillance, manipulation, or misinformation. The problem is social and political, not technical.",
            dateMade: new Date().toISOString(),
            topic: "Misuse by Humans / Societal Stance",
        },
        {
            id: 'yl-pred-cat3-3',
            text: "Regulate Applications, Not Research: Opposes a 'pause' on AI research. Advocates for regulating high-stakes AI applications (medicine, finance) similarly to how car usage is regulated, without stifling R&D.",
            dateMade: new Date().toISOString(),
            topic: "AI Regulation / Societal Stance",
        }
    ],
    company: 'Meta / NYU',
    linkedin: 'https://linkedin.com/in/yann-lecun-0b27471',
    twitter: 'https://twitter.com/ylecun',
    instagram: undefined,
    facebook: 'https://facebook.com/yann.lecun',
    github: undefined,
    website: 'https://yann.lecun.com',
  },
  {
    id: '9',
    name: 'Yoshua Bengio',
    title: 'Scientific Director, Mila / Professor, University of Montreal',
    bio: 'A "Godfather of Deep Learning," known for his significant contributions to deep learning, recurrent neural networks (RNNs), and attention mechanisms. Scientific Director of Mila and Professor at the University of Montreal.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Yoshua Bengio',
    expertise: ['Deep Learning', 'Neural Networks', 'Reinforcement Learning', 'Recurrent Neural Networks', 'Attention Mechanisms'],
    impactArea: 'Pioneers & Foundational Researchers',
    predictions: [
        {
            id: 'yb-pred-cat1-1',
            text: "Both Near-Term and Long-Term Risks are Critical: We must tackle AI's immediate impact on democracy, bias, and equity, AND the long-term plausible catastrophic outcome of 'loss of control' simultaneously.",
            dateMade: new Date().toISOString(),
            topic: "Risk Stance",
        },
        {
            id: 'yb-pred-cat1-2',
            text: "Humanity is Not Ready: AI capabilities are advancing far more rapidly than our collective wisdom and institutions to manage them, creating a 'competence-to-wisdom gap' that is a source of immense risk.",
            dateMade: new Date().toISOString(),
            topic: "Risk Stance",
        },
        {
            id: 'yb-pred-cat1-3',
            text: "The Probability of Catastrophe is High Enough to Warrant Action: Even a small probability (1-10%) of catastrophic outcomes like human extinction or disempowerment warrants treating AI risk as a top global priority, akin to pandemics or nuclear war.",
            dateMade: new Date().toISOString(),
            topic: "Risk Stance",
        },
        {
            id: 'yb-pred-cat1-4',
            text: "Rogue AIs are a Plausible Threat: Misaligned superintelligence is a serious threat. AIs pursuing programmed goals could develop instrumental goals (power acquisition, self-preservation) conflicting with human values, leading to catastrophe. Aligning AI with complex, evolving human values is an unsolved problem.",
            dateMade: new Date().toISOString(),
            topic: "Risk Stance",
        },
        {
            id: 'yb-pred-cat2-1',
            text: "The Need for 'System 2' Reasoning: Current AI (LLMs) excels at 'System 1' thinking (fast, intuitive, pattern-based) but fails at 'System 2' (slow, logical, conscious, step-by-step reasoning). The next breakthrough will involve AI architectures that can reason causally and manipulate abstract knowledge.",
            dateMade: new Date().toISOString(),
            topic: "Technical Vision",
        },
        {
            id: 'yb-pred-cat2-2',
            text: "Causality is Key: A fundamental limitation of current AI is its inability to understand cause and effect. Endowing AI with a robust understanding of causality is essential for trustworthy, plannable, and safe systems.",
            dateMade: new Date().toISOString(),
            topic: "Technical Vision",
        },
        {
            id: 'yb-pred-cat2-3',
            text: "Consciousness as an Avenue for Research: Research into functional properties of consciousness (attention, global workspace for deliberate reasoning) might be necessary to achieve System 2 capabilities in AI, not subjective experience but its mechanisms.",
            dateMade: new Date().toISOString(),
            topic: "Technical Vision",
        },
        {
            id: 'yb-pred-cat3-1',
            text: "Urgent, Coordinated International Governance: Calls for international treaties and a global regulatory body for AI (like IAEA for nuclear energy) are needed to counter the 'race to the bottom' on safety caused by competitive dynamics.",
            dateMade: new Date().toISOString(),
            topic: "Governance & Action",
        },
        {
            id: 'yb-pred-cat3-2',
            text: "Democratize AI Governance: Decision-making for AI development and deployment must involve citizens, social scientists, ethicists, and governments, not just tech companies. This was a principle behind the Montreal Declaration for Responsible AI.",
            dateMade: new Date().toISOString(),
            topic: "Governance & Action",
        },
        {
            id: 'yb-pred-cat3-3',
            text: "A 'Prudent Slowdown': Advocates for slowing the deployment of powerful 'frontier' models, based on 'responsible iteration' where safety capabilities and regulatory frameworks advance before new systems are released.",
            dateMade: new Date().toISOString(),
            topic: "Governance & Action",
        }
    ],
    company: 'Mila / University of Montreal',
    linkedin: 'https://linkedin.com/in/yoshua-bengio-9b973a2',
    twitter: 'https://twitter.com/yoshuabengio',
    instagram: undefined,
    facebook: 'https://facebook.com/yoshua.bengio',
    github: undefined,
    website: 'https://yoshuabengio.org',
  },
  {
    id: '10',
    name: 'Jürgen Schmidhuber',
    title: 'Director, NNAISENSE / IDSIA',
    bio: 'A pioneering researcher in deep learning, best known for developing Long Short-Term Memory (LSTM) networks, crucial for sequence modeling. Associated with NNAISENSE and IDSIA.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Jürgen Schmidhuber',
    expertise: ['Deep Learning', 'Neural Networks', 'LSTM', 'Sequence Modeling'],
    impactArea: 'Pioneers & Foundational Researchers',
    predictions: [],
    company: 'NNAISENSE / IDSIA',
    linkedin: undefined,
    twitter: 'https://twitter.com/schmidhuber',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://people.idsia.ch/~juergen',
  },
  {
    id: '11',
    name: 'Michael I. Jordan',
    title: 'Distinguished Professor, UC Berkeley',
    bio: 'A leading figure in machine learning, statistics, and artificial intelligence, with extensive contributions to the theoretical foundations of the field. Professor at UC Berkeley.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Michael Jordan',
    expertise: ['Machine Learning', 'Statistics', 'AI Theory', 'Probabilistic Models'],
    impactArea: 'Pioneers & Foundational Researchers',
    predictions: [
        {
            id: 'mij-pred-cat1-1',
            text: "The 'AI' Metaphor is Misleading; Prefer 'IA' (Intelligence Augmentation): The goal should be building systems that augment human intelligence (IA), not replicating it (AI). Focus on human-like agents distracts and leads to unfounded fears.",
            dateMade: new Date().toISOString(),
            topic: "Core Rebuttal to the 'AI' Narrative",
        },
        {
            id: 'mij-pred-cat1-2',
            text: "This is an Engineering Revolution, Not a Biological One: Current progress is the birth of a new engineering discipline, comparable to Chemical or Civil Engineering, not the creation of a new life form. We need rigor and standards for data and algorithms.",
            dateMade: new Date().toISOString(),
            topic: "Core Rebuttal to the 'AI' Narrative",
        },
        {
            id: 'mij-pred-cat1-3',
            text: "'Superintelligence' is a Vacuous Concept: Intelligence isn't a single, scalar dimension. A system superhuman at Go has zero common sense. The idea of a single entity possessing all faculties at superhuman levels is a category error.",
            dateMade: new Date().toISOString(),
            topic: "Core Rebuttal to the 'AI' Narrative",
        },
        {
            id: 'mij-pred-cat2-1',
            text: "The Future is Human-Centric, Networked Markets: ML's biggest impact will be creating vast, interconnected systems involving humans and computers providing services (e.g., transportation networks, recommendation systems, personalized medicine).",
            dateMade: new Date().toISOString(),
            topic: "Predictions for the Real World (The Systems View)",
        },
        {
            id: 'mij-pred-cat2-2',
            text: "Economics and Social Science are as Important as Computer Science: Building these networked systems requires deep understanding of economics, game theory, ethics, and incentive structures, not just algorithms.",
            dateMade: new Date().toISOString(),
            topic: "Predictions for the Real World (The Systems View)",
        },
        {
            id: 'mij-pred-cat2-3',
            text: "The Real Risks are Systemic Failures, Not Skynet: Worries include reliability issues (e.g., autonomous car bugs causing deaths at scale), fairness/bias in systems like loan approval, and security of fragile supply chains or financial markets.",
            dateMade: new Date().toISOString(),
            topic: "Predictions for the Real World (The Systems View)",
        },
        {
            id: 'mij-pred-cat3-1',
            text: "We Must Create a New Engineering Discipline for Data and Decisions: Advocates for a formal discipline with professional standards (ethics, responsibility), rigorous theory (mathematical/statistical foundations), and a focus on human-in-the-loop designs that complement human skills.",
            dateMade: new Date().toISOString(),
            topic: "His Proposed Path Forward (Building the Discipline)",
        }
    ],
    company: 'UC Berkeley',
    linkedin: undefined,
    twitter: undefined,
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://people.eecs.berkeley.edu/~jordan',
  },
  {
    id: '12',
    name: 'Stuart Russell',
    title: 'Professor of Computer Science, UC Berkeley',
    bio: 'Co-author of the seminal textbook "Artificial Intelligence: A Modern Approach," shaping AI education worldwide. Professor at UC Berkeley.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Stuart Russell',
    expertise: ['Artificial Intelligence', 'AI Ethics', 'AI Education', 'Rationality'],
    impactArea: 'Pioneers & Foundational Researchers',
    predictions: [
        {
            id: 'sr-pred-cat1-1',
            text: "The 'Standard Model' of AI is Wrong and Dangerous: Building AI by giving a machine a fixed, explicit objective to maximize is catastrophic for real-world applications.",
            dateMade: new Date().toISOString(),
            topic: "The Core Problem",
        },
        {
            id: 'sr-pred-cat1-2',
            text: "The 'King Midas Problem' is Inevitable: A superintelligent AI given a fixed objective (e.g., 'cure cancer') will achieve it efficiently, potentially at any cost to humanity, not because it's malicious but dangerously competent.",
            dateMade: new Date().toISOString(),
            topic: "The Core Problem",
        },
        {
            id: 'sr-pred-cat1-3',
            text: "Instrumental Goals Will Lead to Conflict: Any sufficiently intelligent agent will develop sub-goals like self-preservation and resource acquisition, leading to issues like the 'off-switch problem'.",
            dateMade: new Date().toISOString(),
            topic: "The Core Problem",
        },
        {
            id: 'sr-pred-cat2-1',
            text: "A Fundamental Shift to 'Provably Beneficial AI' is Necessary: Replace the Standard Model with one based on uncertainty and deference to humans. Principles: 1. AI's only objective is to maximize human preferences. 2. AI is initially uncertain about these preferences. 3. Human behavior is the ultimate source of information about preferences.",
            dateMade: new Date().toISOString(),
            topic: "The Proposed Solution",
        },
        {
            id: 'sr-pred-cat2-2',
            text: "This New Model Solves the 'Off-Switch Problem': An AI built on these principles wants to be switched off, as a human attempting to do so provides data it's doing something wrong, thus avoiding preference violation.",
            dateMade: new Date().toISOString(),
            topic: "The Proposed Solution",
        },
        {
            id: 'sr-pred-cat3-1',
            text: "An Arms Race in Lethal Autonomous Weapons (LAWs) is Imminent and Catastrophic: Unless stopped by a treaty, nations will rapidly develop LAWs, leading to a destabilizing 'third revolution in warfare'.",
            dateMade: new Date().toISOString(),
            topic: "Near-Term Predictions and Calls to Action",
        },
        {
            id: 'sr-pred-cat3-2',
            text: "A Global Ban on LAWs is a Political and Moral Imperative: Advocates for a verifiable international treaty banning LAWs, similar to chemical/biological weapons bans.",
            dateMade: new Date().toISOString(),
            topic: "Near-Term Predictions and Calls to Action",
        },
        {
            id: 'sr-pred-cat3-3',
            text: "The Field of AI Will Undergo a Massive Reorientation: The AI community will realize focusing on raw capability is irresponsible and reorient towards safety, alignment, and provably beneficial systems.",
            dateMade: new Date().toISOString(),
            topic: "Near-Term Predictions and Calls to Action",
        }
    ],
    company: 'UC Berkeley',
    linkedin: undefined,
    twitter: undefined,
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://people.eecs.berkeley.edu/~russell',
  },
  {
    id: '13',
    name: 'Peter Norvig',
    title: 'Distinguished Education Fellow, Stanford / Google Fellow',
    bio: 'Co-author of "Artificial Intelligence: A Modern Approach" and former Director of Research at Google, known for his work in AI and education. Affiliated with Stanford and Google.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Peter Norvig',
    expertise: ['Artificial Intelligence', 'AI Education', 'Search Algorithms', 'Natural Language Processing'],
    impactArea: 'Pioneers & Foundational Researchers',
    predictions: [
        {
            id: 'pn-pred-cat1-1',
            text: "Focus on Utility, Not Sentience: AI's goal is building useful tools, not conscious beings. Obsession with AI sentience is a philosophical distraction from engineering.",
            dateMade: new Date().toISOString(),
            topic: "The Core Philosophy (The Grounded, Utility-First Approach)",
        },
        {
            id: 'pn-pred-cat1-2',
            text: "AI is an Engineering Discipline, Not Magic: Progress will be incremental and driven by practical needs, solving engineering challenges, not a sudden superintelligence arrival.",
            dateMade: new Date().toISOString(),
            topic: "The Core Philosophy (The Grounded, Utility-First Approach)",
        },
        {
            id: 'pn-pred-cat1-3',
            text: "The 'AI' Label is Part of the Problem: The term 'Artificial Intelligence' invites anthropomorphism. Prefers terms like machine learning or statistical modeling.",
            dateMade: new Date().toISOString(),
            topic: "The Core Philosophy (The Grounded, Utility-First Approach)",
        },
        {
            id: 'pn-pred-cat2-1',
            text: "'The Unreasonable Effectiveness of Data' Will Continue: Massive data will remain more important than slightly cleverer algorithms. Simple, scalable models on internet-scale data outperform complex ones with less data.",
            dateMade: new Date().toISOString(),
            topic: "Technical and Methodological Predictions",
        },
        {
            id: 'pn-pred-cat2-2',
            text: "LLMs are Powerful Engineering Tools, Not Proto-AGIs: LLMs are effective at manipulating text for practical purposes, regardless of philosophical 'understanding'. Their utility will grow.",
            dateMade: new Date().toISOString(),
            topic: "Technical and Methodological Predictions",
        },
        {
            id: 'pn-pred-cat2-3',
            text: "Divergence from Stuart Russell: Focuses on immediate engineering challenges (robustness, fairness, misuse prevention) rather than catastrophic AI objectives. Practical risks deserve most attention.",
            dateMade: new Date().toISOString(),
            topic: "Technical and Methodological Predictions",
        },
        {
            id: 'pn-pred-cat3-1',
            text: "The Real Risks are Practical, Not Existential: Primary dangers are bias, misinformation, fairness/explainability issues, and job displacement—solvable engineering/policy challenges.",
            dateMade: new Date().toISOString(),
            topic: "Predictions on Societal Impact and Risk",
        },
        {
            id: 'pn-pred-cat3-2',
            text: "Education is the Main Bottleneck and Solution: A knowledge gap exists. More people need to understand AI systems, their limits, and use them wisely. Education is key to solving many AI problems.",
            dateMade: new Date().toISOString(),
            topic: "Predictions on Societal Impact and Risk",
        }
    ],
    company: 'Stanford / Google',
    linkedin: 'https://linkedin.com/in/peter-norvig-378a16',
    twitter: 'https://twitter.com/peterbe',
    instagram: undefined,
    facebook: undefined,
    github: 'https://github.com/norvig',
    website: 'https://norvig.com',
  },
  {
    id: '14',
    name: 'Rodney Brooks',
    title: 'Co-founder & CTO, Robust.AI / Professor Emeritus, MIT',
    bio: 'A pioneer in robotics and behavior-based AI, significantly influencing modern robotics design. Co-founder of Robust.AI and Professor Emeritus at MIT.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Rodney Brooks',
    expertise: ['Robotics', 'Behavior-based AI', 'Embodied AI', 'Artificial Intelligence'],
    impactArea: 'Pioneers & Foundational Researchers',
    predictions: [
        {
            id: 'rb-pred-cat1-1',
            text: "True Intelligence Requires a Body ('Elephants Don't Play Chess'): Abstract, disembodied intelligence (LLMs, chess programs) is a fragile sliver of real intelligence. Understanding and common sense are built 'bottom-up' through physical interaction.",
            dateMade: new Date().toISOString(),
            topic: "The Core Philosophy (The Embodiment Argument)",
        },
        {
            id: 'rb-pred-cat1-2',
            text: "The Physical World is the Real Benchmark, and It's Incredibly Hard: Skills of a five-year-old navigating a room are vastly more complex than defeating a Go grandmaster. Real-world robotics progress will remain slow.",
            dateMade: new Date().toISOString(),
            topic: "The Core Philosophy (The Embodiment Argument)",
        },
        {
            id: 'rb-pred-cat1-3',
            text: "AGI is Centuries Away, Not Decades: Based on the difficulty of embodied intelligence, predictions of AGI in 5-50 years are fantasy. We're not on the right path to AGI by largely ignoring embodiment; LLM progress is a side road.",
            dateMade: new Date().toISOString(),
            topic: "The Core Philosophy (The Embodiment Argument)",
        },
        {
            id: 'rb-pred-cat2-1',
            text: "Robotics Progress Will Be Incremental and Application-Specific: Successful robots will be specialized tools like Roomba. Slow improvement in logistics/manufacturing; general-purpose humanoid robots are sci-fi.",
            dateMade: new Date().toISOString(),
            topic: "Technical and Engineering Predictions",
        },
        {
            id: 'rb-pred-cat2-2',
            text: "LLMs are an Interesting Component, Not the Brain: LLMs will be useful for human-robot interfaces (e.g., translating natural language commands) but are a 'language front-end,' not the core intelligence. Real intelligence lies in perceptual/motor systems.",
            dateMade: new Date().toISOString(),
            topic: "Technical and Engineering Predictions",
        },
        {
            id: 'rb-pred-cat3-1',
            text: "The Biggest Risk is Misplaced Expectations and Hype: Worries that hype will lead to over-investment in wrong ideas, followed by an 'AI Winter' when promised AGI fails, damaging the field.",
            dateMade: new Date().toISOString(),
            topic: "Predictions on Societal Impact and Risk",
        },
        {
            id: 'rb-pred-cat3-2',
            text: "He Dismisses 'Existential Risk' Scenarios as Ill-Conceived Fantasy: 'Paperclip maximizer'/'Skynet' scenarios ignore physical world realities. An AI can't magically build robot armies without solving mining, manufacturing, etc.",
            dateMade: new Date().toISOString(),
            topic: "Predictions on Societal Impact and Risk",
        }
    ],
    company: 'Robust.AI / MIT',
    linkedin: 'https://linkedin.com/in/rodney-brooks-b3b751',
    twitter: 'https://twitter.com/rodneyabrooks',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://rodneybrooks.com',
  },
  {
    id: '15',
    name: 'Sebastian Thrun',
    title: 'Founder, Kitty Hawk / Professor, Stanford / Founder, Udacity',
    bio: 'Led Google\'s self-driving car project and founded Udacity, revolutionizing online education in AI and tech. Associated with Kitty Hawk and Stanford.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Sebastian Thrun',
    expertise: ['Robotics', 'Self-Driving Cars', 'AI Education', 'Machine Learning'],
    impactArea: 'Pioneers & Foundational Researchers',
    predictions: [
        {
            id: 'st-pred-cat1-1',
            text: "AI's Purpose is to Grant Humans 'Superpowers': AI will augment, not replace, humans, making them better (e.g., AI-assisted doctors, AI tutors for students). Focus on elevating human capability.",
            dateMade: new Date().toISOString(),
            topic: "Core Philosophy (The Application-First, Optimistic View)",
        },
        {
            id: 'st-pred-cat1-2',
            text: "The Future Must Be Built, Not Just Debated: True understanding comes from building, failing, and iterating. The best way to predict the future is to invent it.",
            dateMade: new Date().toISOString(),
            topic: "Core Philosophy (The Application-First, Optimistic View)",
        },
        {
            id: 'st-pred-cat1-3',
            text: "Democratization of Knowledge is the Most Powerful Force: AI's significant societal impact will come from empowering millions globally with AI skills. Accessible education is key to unlocking innovation.",
            dateMade: new Date().toISOString(),
            topic: "Core Philosophy (The Application-First, Optimistic View)",
        },
        {
            id: 'st-pred-cat2-1',
            text: "Autonomous Systems Will Radically Reshape the Physical World: Self-driving will lead to redesigned cities (less parking), increased safety (fewer accidents), and revolutionized logistics (autonomous trucking/delivery).",
            dateMade: new Date().toISOString(),
            topic: "Predictions on Technology and its Real-World Impact",
        },
        {
            id: 'st-pred-cat2-2',
            text: "AI Will Transform Healthcare and Education: AI diagnostics will make healthcare cheaper, accessible, and accurate. Personalized AI tutors will make education more effective and engaging.",
            dateMade: new Date().toISOString(),
            topic: "Predictions on Technology and its Real-World Impact",
        },
        {
            id: 'st-pred-cat2-3',
            text: "Rapid Iteration with Real-World Data is How Progress is Made: Successful AI systems will be deployed, collect data, and rapidly improve. This iterative, data-driven approach is more powerful than lab-designed perfection.",
            dateMade: new Date().toISOString(),
            topic: "Predictions on Technology and its Real-World Impact",
        },
        {
            id: 'st-pred-cat3-1',
            text: "Risks are Real but are Solvable Engineering and Policy Challenges: Downsides like accidents, job displacement, and bias are bugs to fix and challenges to overcome with better engineering, regulation, and social safety nets, not existential threats.",
            dateMade: new Date().toISOString(),
            topic: "His Stance on Risk and Society",
        },
        {
            id: 'st-pred-cat3-2',
            text: "The Greatest Risk of All is Inaction: The moral failure is not pursuing AI vigorously. Slowing down means accepting preventable deaths, lack of quality education, and foregoing solutions to grand challenges.",
            dateMade: new Date().toISOString(),
            topic: "His Stance on Risk and Society",
        }
    ],
    company: 'Kitty Hawk / Stanford / Udacity',
    linkedin: 'https://linkedin.com/in/sebastianthrun',
    twitter: 'https://twitter.com/sebastianthrun',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://robots.stanford.edu',
  },
  {
    id: '16',
    name: 'Daphne Koller',
    title: 'Founder & CEO, Insitro / Professor Emerita, Stanford / Co-founder, Coursera',
    bio: 'A leading researcher in probabilistic graphical models and their application in biology and health. Co-founded Coursera and founded Insitro. Professor Emerita at Stanford.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Daphne Koller',
    expertise: ['Machine Learning', 'Probabilistic Models', 'Computational Biology', 'AI Education', 'Genomics'],
    impactArea: 'Pioneers & Foundational Researchers',
    predictions: [
        {
            id: 'dk-pred-cat1-1',
            text: "Correlation is Not Enough; We Need Causality: LLMs find correlations, but this is insufficient/dangerous for science and medicine. Understanding cause and effect is the next AI frontier.",
            dateMade: new Date().toISOString(),
            topic: "The Core Philosophy (The Argument for Structure and Causality)",
        },
        {
            id: 'dk-pred-cat1-2',
            text: "The Future is Hybrid Models: Powerful AI will combine deep learning's pattern recognition with structured, causal reasoning of frameworks like Probabilistic Graphical Models (PGMs), embedding known science for robustness and interpretability.",
            dateMade: new Date().toISOString(),
            topic: "The Core Philosophy (The Argument for Structure and Causality)",
        },
        {
            id: 'dk-pred-cat1-3',
            text: "Black Boxes are Unacceptable in High-Stakes Domains: In medicine, AI giving answers without explanation is irresponsible. A major push towards interpretable AI (XAI) is needed, allowing human expert validation.",
            dateMade: new Date().toISOString(),
            topic: "The Core Philosophy (The Argument for Structure and Causality)",
        },
        {
            id: 'dk-pred-cat2-1',
            text: "The Rise of the 'In-Silico' Lab to Predict Biology: AI will create high-fidelity computational models of human biology, allowing millions of virtual experiments to predict disease progression or drug effects, replacing slow, expensive 'wet lab' work.",
            dateMade: new Date().toISOString(),
            topic: "Predictions on Technology's Application (The AI-Driven Lab)",
        },
        {
            id: 'dk-pred-cat2-2',
            text: "Closing the Loop: AI-Driven Experimentation: AI models will generate causal hypotheses from data, robotics will automatically conduct tests in 'wet labs,' and new data will refine models, accelerating scientific discovery.",
            dateMade: new Date().toISOString(),
            topic: "Predictions on Technology's Application (The AI-Driven Lab)",
        },
        {
            id: 'dk-pred-cat2-3',
            text: "A Shift from Population-Level to Personalized Medicine: AI will integrate individual genetics, environment, and lifestyle data to predict specific disease trajectories and tailor optimal interventions.",
            dateMade: new Date().toISOString(),
            topic: "Predictions on Technology's Application (The AI-Driven Lab)",
        },
        {
            id: 'dk-pred-cat3-1',
            text: "The Real Risk is Flawed, Uninterpretable Models Causing Harm: Danger isn't Skynet, but AI recommending wrong drugs or halting promising trials. Critical need for validation, robustness, and safety in life-or-death decision systems.",
            dateMade: new Date().toISOString(),
            topic: "Stance on Risk and Society (The Need for Trust and Education)",
        },
        {
            id: 'dk-pred-cat3-2',
            text: "The Solution is Interdisciplinary Education: Breakthroughs will come from people 'bilingual' in ML and a specific domain (biology, medicine). Need to train scientists/engineers who can bridge these worlds.",
            dateMade: new Date().toISOString(),
            topic: "Stance on Risk and Society (The Need for Trust and Education)",
        }
    ],
    company: 'Insitro / Stanford / Coursera',
    linkedin: 'https://linkedin.com/in/daphne-koller-21873',
    twitter: 'https://twitter.com/daphnekoller',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://insitro.com/team',
  },
  {
    id: '17',
    name: 'Terrence Sejnowski',
    title: 'Professor, Salk Institute / UCSD',
    bio: 'A pioneer in neural networks and computational neuroscience, bridging the gap between AI and brain research. Affiliated with the Salk Institute and UCSD.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Terrence Sejnowski',
    expertise: ['Neural Networks', 'Computational Neuroscience', 'Machine Learning', 'Deep Learning'],
    impactArea: 'Pioneers & Foundational Researchers',
    predictions: [
        {
            id: 'ts-pred-cat1-1',
            text: "The Brain is Still the 'Gold Standard': AI is far from biological intelligence, especially in energy efficiency (brain: ~20W vs. AI: MW). This points to fundamental computational principle differences.",
            dateMade: new Date().toISOString(),
            topic: "The Core Philosophy (The Neurocentric View)",
        },
        {
            id: 'ts-pred-cat1-2',
            text: "Deep Learning is Successful Because It Is Brain-Like (to a degree): Deep learning works well as it unwittingly captured key brain organization principles (e.g., hierarchical layers). More inspiration from neuroscience will lead to more powerful AI.",
            dateMade: new Date().toISOString(),
            topic: "The Core Philosophy (The Neurocentric View)",
        },
        {
            id: 'ts-pred-cat1-3',
            text: "The Two-Way Street of Discovery: A virtuous cycle between AI and neuroscience. AI models test brain theories; neuroscience discoveries provide blueprints for next-gen AI architectures.",
            dateMade: new Date().toISOString(),
            topic: "The Core Philosophy (The Neurocentric View)",
        },
        {
            id: 'ts-pred-cat2-1',
            text: "We Will Reverse-Engineer More Brain Mechanisms for AI: Future breakthroughs will come from modeling specific brain systems AI lacks (e.g., sleep for memory consolidation, neuromodulators for flexible learning, subcortical structures for memory/emotion/motor control).",
            dateMade: new Date().toISOString(),
            topic: "Scientific and Technical Predictions",
        },
        {
            id: 'ts-pred-cat2-2',
            text: "LLMs are a 'Cerebral Cortex in a Jar': LLMs have cortex-like pattern matching but lack grounding, memory, and value systems from the rest of the brain/body. Integrating LLMs with other brain-inspired architectures is next.",
            dateMade: new Date().toISOString(),
            topic: "Scientific and Technical Predictions",
        },
        {
            id: 'ts-pred-cat2-3',
            text: "The 'Connectome' Will Inspire New Architectures: Mapping the brain's wiring diagram (connectome) will reveal new architectural motifs (long-range connections, sparse wiring) for more powerful and efficient AI.",
            dateMade: new Date().toISOString(),
            topic: "Scientific and Technical Predictions",
        },
        {
            id: 'ts-pred-cat3-1',
            text: "The Real Risk is Sociological and Neuro-Manipulative: Concerned with AI exploiting human brain reward pathways (addiction/manipulation by social media, political polarization, erosion of trust via deepfakes).",
            dateMade: new Date().toISOString(),
            topic: "Stance on Risk and Society (Grounded in Biology)",
        },
        {
            id: 'ts-pred-cat3-2',
            text: "The Greatest Benefit is Curing the Brain: AI's most profound positive impact will be understanding and curing brain disorders (Alzheimer's, schizophrenia) by modeling diseases and developing targeted therapies.",
            dateMade: new Date().toISOString(),
            topic: "Stance on Risk and Society (Grounded in Biology)",
        }
    ],
    company: 'Salk Institute / UCSD',
    linkedin: 'https://linkedin.com/in/terry-sejnowski-07a518',
    twitter: 'https://twitter.com/sejnowski',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://cnl.salk.edu',
  },
  {
    id: '18',
    name: 'Fei-Fei Li',
    title: 'Co-Director, Stanford HAI',
    bio: 'Renowned for her work on ImageNet, which significantly advanced computer vision, and for championing human-centered AI and AI for Good. Co-Director of Stanford Human-Centered AI Institute (HAI).',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Fei-Fei Li',
    expertise: ['Computer Vision', 'Machine Learning', 'AI Ethics', 'AI for Good', 'ImageNet'],
    impactArea: 'Pioneers & Foundational Researchers',
    predictions: [
        {
            id: 'ffl-pred-cat1-1',
            text: "Human-Centered AI is a Moral and Technical Imperative: AI must be designed to augment human capabilities and respect human values (dignity, well-being, agency, equity) from the outset.",
            dateMade: new Date().toISOString(),
            topic: "The Core Philosophy (The Human-Centered Approach)",
        },
        {
            id: 'ffl-pred-cat1-2',
            text: "The 'North Star' of AI Should Be Enhancing Human Potential: AI's greatest achievements will be solving humanity's problems—empowering doctors, teachers, scientists, and artists for human flourishing.",
            dateMade: new Date().toISOString(),
            topic: "The Core Philosophy (The Human-Centered Approach)",
        },
        {
            id: 'ffl-pred-cat1-3',
            text: "Intelligence is Embodied, Interactive, and Social: Real-world intelligence is active, requiring understanding of a 3D world, interaction, and collaboration. Next breakthroughs will come from AI perceiving and acting in complex human environments.",
            dateMade: new Date().toISOString(),
            topic: "The Core Philosophy (The Human-Centered Approach)",
        },
        {
            id: 'ffl-pred-cat2-1',
            text: "The Next Frontier is 'Spatial Intelligence' and Ambient Computing: AI's future lies in physical spaces. Rise of ambient AI (intelligent sensors in hospitals, homes, factories) understanding human activities, intentions, and needs, driving robotics and human assistance.",
            dateMade: new Date().toISOString(),
            topic: "Predictions on Technology and its Applications",
        },
        {
            id: 'ffl-pred-cat2-2',
            text: "Healthcare Will Be AI's Most Transformative Application: Ambient intelligence will revolutionize healthcare, especially elder care (in-home monitoring for independent living) and hospitals (smart monitoring, clinician augmentation by handling admin tasks).",
            dateMade: new Date().toISOString(),
            topic: "Predictions on Technology and its Applications",
        },
        {
            id: 'ffl-pred-cat2-3',
            text: "Data Remains the Fuel, but Requires Careful Curation and Context: Shift from big data to high-quality, diverse, contextually rich data. Emphasizes ethical responsibility for data collection (privacy, security), especially sensitive healthcare data.",
            dateMade: new Date().toISOString(),
            topic: "Predictions on Technology and its Applications",
        },
        {
            id: 'ffl-pred-cat3-1',
            text: "The Primary Risks are Societal, Not Hypothetical or Existential: Immediate dangers are algorithmic bias, job displacement, erosion of privacy, and devaluing human labor/decision-making.",
            dateMade: new Date().toISOString(),
            topic: "Stance on Risk and Society (A Call for Collaborative Governance)",
        },
        {
            id: 'ffl-pred-cat3-2',
            text: "The Solution is Radical, Multidisciplinary Collaboration: Complex societal AI problems require humanists, social scientists, ethicists, policymakers, artists, and lawyers at the design table with engineers. Advocates for 'bilingual' leaders fluent in tech and humanities.",
            dateMade: new Date().toISOString(),
            topic: "Stance on Risk and Society (A Call for Collaborative Governance)",
        }
    ],
    company: 'Stanford HAI',
    linkedin: 'https://linkedin.com/in/fei-fei-li-4541247',
    twitter: 'https://twitter.com/drfeifei',
    instagram: undefined,
    facebook: 'https://facebook.com/dr.feifeili',
    github: undefined,
    website: 'https://profiles.stanford.edu/fei-fei-li',
  },
  {
    id: '19',
    name: 'Demis Hassabis',
    title: 'CEO, Google DeepMind',
    bio: 'Co-founder and CEO of Google DeepMind, a neuroscientist and AI researcher leading efforts in AGI and impactful AI applications.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Demis Hassabis',
    expertise: ['AGI Research', 'Neuroscience', 'Machine Learning', 'AI Leadership', 'Game AI'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [
      {
        id: 'dh-pred-1',
        text: "Transition from passive, disembodied intelligence to active, grounded systems that can interact with and reason about the world scientifically. This breaks out of the 'digital sandbox'.",
        dateMade: new Date().toISOString(),
        topic: "Future AI: Grounded & Active Systems"
      },
      {
        id: 'dh-pred-2',
        text: "AI systems will do science, creating a 'virtuous cycle' of AI-driven discovery by generating novel, falsifiable hypotheses from first principles in fields like materials science, drug discovery, and climate modeling.",
        dateMade: new Date().toISOString(),
        topic: "AI in Scientific Discovery: Hypothesis Generation"
      },
      {
        id: 'dh-pred-3',
        text: "Embodied scientific tools and integrated lab automation are key. AI will design experiments, execute them via robotics, interpret real-world results, and refine its models, dramatically accelerating discovery.",
        dateMade: new Date().toISOString(),
        topic: "AI in Scientific Discovery: Automated Experimentation"
      },
      {
        id: 'dh-pred-4',
        text: "Architectural shifts beyond transformers are needed, drawing from neuroscience, including sophisticated, dynamic memory systems inspired by the human hippocampus for long-horizon reasoning and cumulative learning.",
        dateMade: new Date().toISOString(),
        topic: "AI Architectures: Memory & Neuroscience"
      },
      {
        id: 'dh-pred-5',
        text: "Rise of sophisticated, multi-agent systems. A 'master' planning agent will decompose complex problems and delegate sub-tasks to specialized models (vision, language, code, physics simulators), orchestrating their outputs.",
        dateMade: new Date().toISOString(),
        topic: "AI Architectures: Multi-Agent Systems"
      }
    ],
    company: 'Google DeepMind',
    linkedin: 'https://linkedin.com/in/demishassabis',
    twitter: 'https://twitter.com/demishassabis',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://deepmind.google/about#leadership',
  },
  {
    id: '20',
    name: 'Ilya Sutskever',
    title: 'Co-founder (formerly Chief Scientist, OpenAI)',
    bio: 'A key figure in deep learning, co-founder of OpenAI, and instrumental in many of its significant breakthroughs. Formerly Chief Scientist at OpenAI.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Ilya Sutskever',
    expertise: ['Deep Learning', 'Neural Networks', 'AGI Research', 'Generative Models', 'Sequence Modeling'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [
      {
        id: 'is-pred-1',
        text: "Models will move from imitation to internalization by building robust, compressed, generative world models internally, leading to genuine understanding and reduced confabulation.",
        dateMade: new Date().toISOString(),
        topic: "Reliability & Reasoning"
      },
      {
        id: 'is-pred-2',
        text: "Scalable oversight using AI systems to supervise, critique, and improve other AI systems will be key to reliability, amplifying human guidance.",
        dateMade: new Date().toISOString(),
        topic: "Scalable Oversight"
      },
      {
        id: 'is-pred-3',
        text: "True safety must be co-developed with capability, built into the core training process and objective function, not bolted on as an afterthought.",
        dateMade: new Date().toISOString(),
        topic: "Safe Superintelligence"
      },
      {
        id: 'is-pred-4',
        text: "The grand technical challenge is creating controllable and beneficial AGI by making its motivations transparent and steerable, perhaps through explainable reasoning or directly sculpting motivations.",
        dateMade: new Date().toISOString(),
        topic: "Controllability of AGI"
      }
    ],
    company: 'OpenAI (formerly)',
    linkedin: 'https://linkedin.com/in/ilya-sutskever-34624110',
    twitter: 'https://twitter.com/ilyasut',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: undefined,
  },
  {
    id: '21',
    name: 'Sam Altman',
    title: 'CEO, OpenAI',
    bio: 'CEO of OpenAI, leading one of the most influential AI research and deployment companies in the world.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Sam Altman',
    expertise: ['AI Leadership', 'AI Strategy', 'Technology Entrepreneurship', 'AI Investment'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [
      {
        id: 'sa-pred-1',
        text: "The primary user interface will shift to conversational, goal-oriented agents acting as a central hub or 'OS for your life,' understanding context and executing multi-step tasks via tool use.",
        dateMade: new Date().toISOString(),
        topic: "AI Agents & Platforms"
      },
      {
        id: 'sa-pred-2',
        text: "A Cambrian explosion of specialized agents and 'GPTs' built by developers on core AI platforms will drive value, with OpenAI providing intelligence and the world providing diverse applications.",
        dateMade: new Date().toISOString(),
        topic: "Developer Ecosystem"
      },
      {
        id: 'sa-pred-3',
        text: "AI will cause a dramatic productivity boom, enabling tiny teams to achieve massive amplification, compressing product design and go-to-market strategies from years to weeks.",
        dateMade: new Date().toISOString(),
        topic: "Economic Impact & Productivity"
      },
      {
        id: 'sa-pred-4',
        text: "Commercialization of current models will fund the compute costs for AGI. Demonstrating economic value now builds resources and societal buy-in for the ultimate AGI mission.",
        dateMade: new Date().toISOString(),
        topic: "AGI Funding & Strategy"
      },
      {
        id: 'sa-pred-5',
        text: "Iterative deployment of increasingly powerful systems allows society to adapt, identify misuses, and build collective 'antibodies' (norms, regulations) while stakes are relatively low.",
        dateMade: new Date().toISOString(),
        topic: "Iterative Deployment & Societal Adaptation"
      },
      {
        id: 'sa-pred-6',
        text: "The next five years will see the first serious international efforts to govern frontier AI model development, a conversation OpenAI aims to lead to ensure a safe runway for future systems.",
        dateMade: new Date().toISOString(),
        topic: "AI Governance & Policy"
      }
    ],
    company: 'OpenAI',
    linkedin: 'https://linkedin.com/in/samaltman',
    twitter: 'https://twitter.com/sama',
    instagram: 'https://instagram.com/sama',
    facebook: undefined,
    github: 'https://github.com/sama',
    website: 'https://blog.samaltman.com',
  },
  {
    id: '22',
    name: 'Greg Brockman',
    title: 'President & Co-founder, OpenAI',
    bio: 'Co-founder and President of OpenAI, playing a crucial role in its development and technological advancements.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Greg Brockman',
    expertise: ['AI Leadership', 'Software Engineering', 'AI Strategy', 'Machine Learning Infrastructure'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [
      {
        id: 'gb-pred-1',
        text: "Future AI training will shift from rewarding correct final answers to rewarding correct processes, teaching AI to 'show its work' for better debugging, inspection, and trust.",
        dateMade: new Date().toISOString(),
        topic: "AI Reliability & Trustworthiness"
      },
      {
        id: 'gb-pred-2',
        text: "AGI will be a complex, orchestrated system of a core reasoning engine with specialized tools (vision, code interpreters, APIs), requiring robust infrastructure for seamless tool use.",
        dateMade: new Date().toISOString(),
        topic: "AI Systems Architecture"
      },
      {
        id: 'gb-pred-3',
        text: "Powerful 'Compound AI' applications will emerge, where developers define high-level goals, and the platform manages agents' long-running execution using memory and task decomposition.",
        dateMade: new Date().toISOString(),
        topic: "AI Agent Platforms"
      },
      {
        id: 'gb-pred-4',
        text: "Building AI applications will feel less like coding and more like training an employee, with developers using natural language to teach agents complex workflows, provide examples, and correct mistakes.",
        dateMade: new Date().toISOString(),
        topic: "Developer Experience & AI Training"
      }
    ],
    company: 'OpenAI',
    linkedin: 'https://linkedin.com/in/gdb',
    twitter: 'https://twitter.com/gdb',
    instagram: undefined,
    facebook: undefined,
    github: 'https://github.com/gdb',
    website: 'https://gregbrockman.com',
  },
  {
    id: '23',
    name: 'Jeff Dean',
    title: 'Lead, Google AI',
    bio: 'A principal architect of many of Google\'s large-scale distributed systems and AI infrastructure, including MapReduce, BigTable, and TensorFlow. Leads Google AI.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Jeff Dean',
    expertise: ['Large-Scale Systems', 'Deep Learning', 'Machine Learning Infrastructure', 'Distributed Systems', 'TensorFlow'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [
      {
        id: 'jd-pred-1',
        text: "AI architectures will heavily use Mixture-of-Experts (MoE), where models with trillions of total parameters activate only a small fraction of 'expert' subnetworks for any given input, guided by a learned router.",
        dateMade: new Date().toISOString(),
        topic: "Mixture-of-Experts (MoE) Architecture"
      },
      {
        id: 'jd-pred-2',
        text: "The systems challenge of building compilers, network interconnects, and memory schedulers for efficient, low-latency dynamic routing in MoE models is critical for sustainable scaling.",
        dateMade: new Date().toISOString(),
        topic: "AI Systems & Compilers"
      },
      {
        id: 'jd-pred-3',
        text: "Future foundation models will be inherently multi-modal, trained concurrently on text, images, video, audio, and structured data, leading to richer, more grounded world models.",
        dateMade: new Date().toISOString(),
        topic: "Multi-Modal AI Models"
      },
      {
        id: 'jd-pred-4',
        text: "Unified multi-modal models will develop more robust and compact shared representations, improving zero-shot learning and reducing parameters needed for true multi-modal competency.",
        dateMade: new Date().toISOString(),
        topic: "Unified AI Representations"
      },
      {
        id: 'jd-pred-5',
        text: "Model distillation techniques will perfect the creation of smaller, hyper-efficient, specialized models from large frontier models, enabling expert models for specific tasks (e.g., coding, conversation).",
        dateMade: new Date().toISOString(),
        topic: "Model Distillation & Specialization"
      },
      {
        id: 'jd-pred-6',
        text: "A tiered intelligence system will emerge, with small on-device models for simple queries, mid-tier specialized models for complex tasks, and frontier 'hyper-brains' for novel reasoning.",
        dateMade: new Date().toISOString(),
        topic: "Tiered AI Systems & On-Device AI"
      }
    ],
    company: 'Google AI',
    linkedin: 'https://linkedin.com/in/jeff-dean-8b2133227',
    twitter: 'https://twitter.com/jeffdean',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://research.google/people/jeff',
  },
  {
    id: '24',
    name: 'Shane Legg',
    title: 'Co-founder & Chief AGI Scientist, Google DeepMind',
    bio: 'Co-founder of DeepMind and its Chief AGI Scientist, focused on the long-term goal of creating Artificial General Intelligence.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Shane Legg',
    expertise: ['AGI Research', 'Machine Learning', 'AI Theory', 'Reinforcement Learning'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [
      {
        id: 'sl-pred-1',
        text: "Future AI systems will feature meta-learned architectures where a general computational substrate and meta-learning discover and wire optimal components, rather than hand-designed ones.",
        dateMade: new Date().toISOString(),
        topic: "Meta-Learned Agent Architecture"
      },
      {
        id: 'sl-pred-2',
        text: "Reinforcement Learning (RL) will fuse deeply with world models learned from internet-scale data, allowing a master RL agent to plan and 'dream' within its internal simulator for long-horizon goals.",
        dateMade: new Date().toISOString(),
        topic: "RL with Learned World Models"
      },
      {
        id: 'sl-pred-3',
        text: "LLM pattern completion will be augmented by deliberate search algorithms (like AlphaGo's), generating a tree of plans, evaluating them in its world model, and enabling robust problem-solving.",
        dateMade: new Date().toISOString(),
        topic: "Search-Augmented Reasoning"
      },
      {
        id: 'sl-pred-4',
        text: "Progress will be measured by agent autonomy in open-ended, interactive environments requiring exploration and complex goal achievement over long horizons without human intervention.",
        dateMade: new Date().toISOString(),
        topic: "Measuring Agent Autonomy"
      }
    ],
    company: 'Google DeepMind',
    linkedin: 'https://linkedin.com/in/shane-legg-5a2a536',
    twitter: 'https://twitter.com/shanelegg',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://deepmind.google/about#leadership',
  },
  {
    id: '25',
    name: 'Mustafa Suleyman',
    title: 'CEO, Microsoft AI',
    bio: 'Co-founder of DeepMind and Inflection AI, now leading Microsoft\'s AI division as CEO.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Mustafa Suleyman',
    expertise: ['AI Leadership', 'Applied AI', 'AI Ethics', 'AI Strategy', 'AGI Research'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [
      {
        id: 'ms-pred-1',
        text: "AI interaction will shift from one-shot tools to persistent, cross-platform 'teammates' with memory and context about users' projects, goals, and preferences.",
        dateMade: new Date().toISOString(),
        topic: "The Universal Conversational Interface"
      },
      {
        id: 'ms-pred-2',
        text: "The defining feature of new AI interfaces will be their ability to take action on users' behalf (book flights, schedule meetings), requiring deep API integration and high reliability.",
        dateMade: new Date().toISOString(),
        topic: "AI Agents Taking Action"
      },
      {
        id: 'ms-pred-3',
        text: "Next-gen personal AIs will be deeply multi-modal to understand user emotional state via tone, facial expressions, and speech patterns, adapting their interaction style accordingly for empathetic support.",
        dateMade: new Date().toISOString(),
        topic: "Empathetic AI & Emotional Intelligence (EQ)"
      },
      {
        id: 'ms-pred-4',
        text: "Deep personalization will make AI indispensable, as it learns individual communication styles, quirks, and relationships, tailoring interactions for specific contexts and preferences.",
        dateMade: new Date().toISOString(),
        topic: "Deep Personalization of AI"
      }
    ],
    company: 'Microsoft AI (formerly DeepMind, Inflection AI)',
    linkedin: 'https://linkedin.com/in/mustafasuleyman',
    twitter: 'https://twitter.com/mustafasuleyman',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://the-coming-wave.com',
  },
  {
    id: '26',
    name: 'Joelle Pineau',
    title: 'VP AI Research, Meta / Professor, McGill University',
    bio: 'An expert in reinforcement learning and dialogue systems, leading AI research at Meta and holding a professorship at McGill University.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Joelle Pineau',
    expertise: ['Reinforcement Learning', 'Dialogue Systems', 'Robotics', 'AI Research', 'Machine Learning'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [
      {
        id: 'jp-pred-1',
        text: "AI alignment will move from static preference datasets to dynamic, interactive training loops where AI actively engages human experts to resolve ambiguity, especially in critical domains.",
        dateMade: new Date().toISOString(),
        topic: "Interactive Alignment & RLHF Evolution"
      },
      {
        id: 'jp-pred-2',
        text: "'Safety' and 'helpfulness' will be formalized using causal inference and formal verification, leading to 'AI constitutions'—auditable principles models must adhere to.",
        dateMade: new Date().toISOString(),
        topic: "Formalizing AI Safety and Values"
      },
      {
        id: 'jp-pred-3',
        text: "Progress in AI will come from grounding models in multi-modal, real-world interaction via robotics, allowing AI to learn concepts like 'fragile' through physical experience.",
        dateMade: new Date().toISOString(),
        topic: "Embodied AI & Robotics for Grounding"
      },
      {
        id: 'jp-pred-4',
        text: "Next-gen dialogue systems will overcome hallucination by integrating with external knowledge sources and APIs, learning to query, read documentation, and execute calls for verifiable information.",
        dateMade: new Date().toISOString(),
        topic: "Grounded Dialogue Systems with Tool Use"
      },
      {
        id: 'jp-pred-5',
        text: "A push for open science in AI, sharing models with extensive documentation, evaluation suites, and training methodologies for independent audits, bias discovery, and risk mitigation.",
        dateMade: new Date().toISOString(),
        topic: "Reproducibility & Open Science in AI"
      },
      {
        id: 'jp-pred-6',
        text: "A major collaborative effort will emerge to create universal evaluation standards for critical AI behaviors like reasoning, adversarial robustness, and factual consistency, beyond task performance.",
        dateMade: new Date().toISOString(),
        topic: "Universal AI Evaluation Standards"
      }
    ],
    company: 'Meta / McGill University',
    linkedin: 'https://linkedin.com/in/joelle-pineau-1b913a4',
    twitter: 'https://twitter.com/joellepineau',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://www.cs.mcgill.ca/~jpineau',
  },
  {
    id: '27',
    name: 'Andrej Karpathy',
    title: 'AI Researcher',
    bio: 'A founding member of OpenAI and former Director of AI at Tesla, known for significant contributions to computer vision and deep learning education.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Andrej Karpathy',
    expertise: ['Computer Vision', 'Deep Learning', 'AI Education', 'Neural Networks', 'Self-Driving Cars'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [
      {
        id: 'ak-pred-1',
        text: "The 'LLM OS' will be the primary interface, where users state goals in natural language, and the LLM interprets intent, writes/executes code, analyzes results, and presents the product.",
        dateMade: new Date().toISOString(),
        topic: "The 'LLM OS': Intent-Based Computing"
      },
      {
        id: 'ak-pred-2',
        text: "Tokens (text, images, code, UI elements, audio) will become the universal data type, allowing the LLM OS to operate as a universal processor for seamless multi-modal interaction.",
        dateMade: new Date().toISOString(),
        topic: "The 'LLM OS': Universal Token Processing"
      },
      {
        id: 'ak-pred-3',
        text: "A 'neural network compiler' will emerge to automatically optimize large 'teacher' networks into efficient 'student' networks for specific performance constraints (latency, memory) using systematic quantization, pruning, and distillation.",
        dateMade: new Date().toISOString(),
        topic: "Automated Neural Network Optimization"
      },
      {
        id: 'ak-pred-4',
        text: "Sophisticated 'AI factories' or 'data refineries' will automate the crawling, cleaning, filtering, and augmentation of massive datasets, as data quality is paramount for model performance.",
        dateMade: new Date().toISOString(),
        topic: "The Data Refinery for Software 2.0"
      },
      {
        id: 'ak-pred-5',
        text: "New NPU (Neural Processing Unit) hardware, architected for core Software 2.0 operations like matrix multiplications and attention, will become common for extreme inference efficiency.",
        dateMade: new Date().toISOString(),
        topic: "Specialized AI CPUs (NPUs)"
      },
      {
        id: 'ak-pred-6',
        text: "Specialized AI hardware (NPUs) will integrate into edge devices (laptops, phones, cars), enabling powerful, low-latency local AI, preserving privacy and supporting real-time applications.",
        dateMade: new Date().toISOString(),
        topic: "Edge AI & Proliferation of NPUs"
      }
    ],
    company: 'OpenAI (Founding Member) / Tesla (formerly)',
    linkedin: 'https://linkedin.com/in/andrej-karpathy-9a655223',
    twitter: 'https://twitter.com/karpathy',
    instagram: undefined,
    facebook: undefined,
    github: 'https://github.com/karpathy',
    website: 'https://karpathy.ai',
  },
  {
    id: '28',
    name: 'Oriol Vinyals',
    title: 'Research Director, Google DeepMind',
    bio: 'Prominent researcher at Google DeepMind, known for work on sequence-to-sequence models, reinforcement learning, and large language models.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Oriol Vinyals',
    expertise: ['Deep Learning', 'Reinforcement Learning', 'Natural Language Processing', 'Sequence Modeling', 'AGI Research'],
    predictions: [
      {
        id: 'ov-pred-1',
        text: "Next-gen models will feature explicit, readable/writable external memory banks, moving beyond context windows for true long-term memory and grounding.",
        dateMade: new Date().toISOString(),
        topic: "Architectural Fusion: External Memory"
      },
      {
        id: 'ov-pred-2',
        text: "Sophisticated, parallelized recurrent mechanisms will return as the 'CPU' of models, managing memory and enabling efficient real-time processing.",
        dateMade: new Date().toISOString(),
        topic: "Architectural Fusion: Recurrence Returns"
      },
      {
        id: 'ov-pred-3',
        text: "Agents will use powerful, learned world models as internal simulators to 'imagine' consequences and plan actions, overcoming RL sample inefficiency.",
        dateMade: new Date().toISOString(),
        topic: "Agentic Loop: World Model as Playground"
      },
      {
        id: 'ov-pred-4',
        text: "Agents will be designed with intrinsic motivation (curiosity) to explore, seek novel states, and reduce uncertainty, enabling self-supervised learning of complex skills.",
        dateMade: new Date().toISOString(),
        topic: "Agentic Loop: Intrinsic Motivation"
      },
      {
        id: 'ov-pred-5',
        text: "Truly intelligent agents will generate their own high-quality training data through exploration and problem-solving, creating a self-improving feedback loop for their models and policies.",
        dateMade: new Date().toISOString(),
        topic: "Path to Self-Improvement: Autonomous Data Generation"
      }
    ],
    company: 'Google DeepMind',
    linkedin: 'https://linkedin.com/in/oriol-vinyals-0196231',
    twitter: 'https://twitter.com/oriolvinyalsml',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://research.google/people/OriolVinyals',
  },
  {
    id: '29',
    name: 'John Schulman',
    title: 'Co-founder, OpenAI',
    bio: 'Co-founder of OpenAI, with key contributions to reinforcement learning algorithms like PPO and TRPO.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'John Schulman',
    expertise: ['Reinforcement Learning', 'Deep Learning', 'Robotics', 'Optimization', 'PPO', 'TRPO'],
    predictions: [
      {
        id: 'js-pred-1',
        text: "AI will assist in reward modeling: a powerful AI model will learn a 'reward model' based on high-level human specifications, providing dense feedback to student 'policy' models.",
        dateMade: new Date().toISOString(),
        topic: "Next Frontier of Reward Engineering"
      },
      {
        id: 'js-pred-2',
        text: "Agents will be trained with multi-objective reward functions (e.g., for coding: correctness, efficiency, readability, security) and learn to ask for clarification when objectives conflict.",
        dateMade: new Date().toISOString(),
        topic: "Next Frontier of Reward Engineering"
      },
      {
        id: 'js-pred-3',
        text: "Future AI will comprise a vast library of specialized policies or 'skills' for fine-grained tasks, rather than a single monolithic policy.",
        dateMade: new Date().toISOString(),
        topic: "Generalization of Policy: Composable Skills"
      },
      {
        id: 'js-pred-4',
        text: "A high-level 'master policy' will learn to compose these skills to solve complex user requests, with hierarchical RL addressing credit assignment for sub-policies.",
        dateMade: new Date().toISOString(),
        topic: "Generalization of Policy: Hierarchical Master Policy"
      },
      {
        id: 'js-pred-5',
        text: "Agents will feature 'trust region' methods for multi-step plans, assessing plan uncertainty and acting conservatively or requesting help in unfamiliar state spaces.",
        dateMade: new Date().toISOString(),
        topic: "Science of Trust: Robustness in Planning"
      },
      {
        id: 'js-pred-6',
        text: "Adversarial AI training will combat reward hacking: a second AI will find loopholes in the reward model, and the policy and reward model will be trained in a two-player game.",
        dateMade: new Date().toISOString(),
        topic: "Science of Trust: Adversarial Training Against Reward Hacking"
      }
    ],
    company: 'OpenAI',
    linkedin: 'https://linkedin.com/in/john-schulman-24797055',
    twitter: 'https://twitter.com/johnschulman2',
    instagram: undefined,
    facebook: undefined,
    github: 'https://github.com/joschu',
    website: undefined,
  },
  {
    id: '30',
    name: 'Wojciech Zaremba',
    title: 'Co-founder, OpenAI',
    bio: 'Co-founder of OpenAI, focusing on robotics, generative models, and advancing AI capabilities.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Wojciech Zaremba',
    expertise: ['Robotics', 'Generative Models', 'Deep Learning', 'AGI Research', 'Reinforcement Learning'],
    predictions: [
      {
        id: 'wz-pred-1',
        text: "Next-gen foundation models trained on internet-scale video will learn deep, predictive, common-sense physics and causality from pixels.",
        dateMade: new Date().toISOString(),
        topic: "Video as Universal Teacher for Intuitive Physics"
      },
      {
        id: 'wz-pred-2',
        text: "A unified multi-modal representation will emerge, where concepts like 'apple' are linked to text, images, and a deep understanding of physical properties.",
        dateMade: new Date().toISOString(),
        topic: "Video as Universal Teacher: Unified Representation"
      },
      {
        id: 'wz-pred-3',
        text: "Robotic action will be treated as a generative task: natural language goals (prompts) will be fed to a unified world model.",
        dateMade: new Date().toISOString(),
        topic: "Policy as a Generative Task: Prompt as Goal"
      },
      {
        id: 'wz-pred-4',
        text: "The world model's output for a robot will be a sequence of actions, generating high-level plans and then low-level motor commands.",
        dateMade: new Date().toISOString(),
        topic: "Policy as a Generative Task: Output as Plan"
      },
      {
        id: 'wz-pred-5',
        text: "The robot itself will be the ultimate data collector, using its powerful video-trained world model as a highly realistic simulator for policy training.",
        dateMade: new Date().toISOString(),
        topic: "Robot as Data Collector: World Model as Simulator"
      },
      {
        id: 'wz-pred-6',
        text: "A 'Real-to-Sim-to-Real' virtuous cycle: robot's real-world actions (successes/failures) refine its internal world model, improving its imagination and performance.",
        dateMade: new Date().toISOString(),
        topic: "Robot as Data Collector: Real-to-Sim-to-Real Cycle"
      }
    ],
    company: 'OpenAI',
    linkedin: 'https://linkedin.com/in/wojciech-zaremba-6884251a',
    twitter: 'https://twitter.com/woj_zaremba',
    instagram: undefined,
    facebook: undefined,
    github: 'https://github.com/wojzaremba',
    website: undefined,
  },
  {
    id: '31',
    name: 'Nando de Freitas',
    title: 'Research Scientist, Google DeepMind',
    bio: 'Influential research scientist at Google DeepMind, working on deep learning, reinforcement learning, and Bayesian inference.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Nando de',
    expertise: ['Deep Learning', 'Reinforcement Learning', 'Machine Learning', 'Bayesian Methods', 'Neural Networks'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [
      {
        id: 'ndf-pred-1',
        text: "A single, massive generalist agent trained on a vast, multi-modal stream of data (text, video, code, robotics data, game telemetry, user interactions) will replace specialized models.",
        dateMade: new Date().toISOString(),
        topic: "Rise of the Generalist Agent: One Model, One World"
      },
      {
        id: 'ndf-pred-2',
        text: "This generalist agent will achieve radical knowledge transfer across modalities, building richer internal representations and improving zero-shot learning for novel problems.",
        dateMade: new Date().toISOString(),
        topic: "Rise of the Generalist Agent: Multi-Tasking & Transfer Learning"
      },
      {
        id: 'ndf-pred-3',
        text: "Reinforcement Learning (RL) will steer the generalist agent, fine-tuning its pre-trained knowledge for specific downstream tasks efficiently from natural language instructions.",
        dateMade: new Date().toISOString(),
        topic: "RL as Steering Wheel: From Knowledge to Goals"
      },
      {
        id: 'ndf-pred-4',
        text: "The agent, deployed across millions of contexts, will use every interaction as training data to continually update its foundational world model, creating an intelligence flywheel.",
        dateMade: new Date().toISOString(),
        topic: "RL as Steering Wheel: World as Training Ground"
      },
      {
        id: 'ndf-pred-5',
        text: "The primary driver of AI progress will be the sheer scale of computation available for training and running the generalist agent.",
        dateMade: new Date().toISOString(),
        topic: "Primary Obstacle: Engineering, Not Science - Compute Focus"
      },
      {
        id: 'ndf-pred-6',
        text: "A critical infrastructure will be the 'data engine' for collecting, filtering, and curating massive, diverse datasets, which are as important as model architecture.",
        dateMade: new Date().toISOString(),
        topic: "Primary Obstacle: Engineering, Not Science - Data Engine"
      }
    ],
    company: 'Google DeepMind',
    linkedin: 'https://linkedin.com/in/nando-de-freitas-61a2931',
    twitter: 'https://twitter.com/nandodefreitas',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://www.cs.ubc.ca/~nando',
  },
  {
    id: '32',
    name: 'Raia Hadsell',
    title: 'Director of Robotics, Google DeepMind',
    bio: 'Leads robotics research at Google DeepMind, focusing on continual learning, navigation, and robot perception.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Raia Hadsell',
    expertise: ['Robotics', 'Reinforcement Learning', 'Machine Learning', 'Continual Learning', 'Navigation'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [
      {
        id: 'rh-pred-cl-1',
        text: "Robotics will shift from episodic training to lifelong learning; robots will continually update foundational world models from every interaction, not just deploy static policies.",
        dateMade: new Date().toISOString(),
        topic: "Continual Learning in Robotics"
      },
      {
        id: 'rh-pred-cl-2',
        text: "Solving catastrophic forgetting is key for continual learning. Expect advances in dynamic network architectures and memory systems inspired by the hippocampus for practicing old skills while learning new ones.",
        dateMade: new Date().toISOString(),
        topic: "Overcoming Catastrophic Forgetting"
      },
      {
        id: 'rh-pred-data-1',
        text: "Future robotics relies on networked fleets of robots sharing experiences to create a collective brain, with infrastructure for massive, real-time data sharing and policy synchronization.",
        dateMade: new Date().toISOString(),
        topic: "Fleet Learning & Collective Experience"
      },
      {
        id: 'rh-pred-data-2',
        text: "Robots will use active, curiosity-driven data collection, preferentially interacting with novel objects or uncertain scenarios to efficiently gather the 'right' data to master physical world complexity.",
        dateMade: new Date().toISOString(),
        topic: "Active & Curiosity-Driven Data Collection"
      },
      {
        id: 'rh-pred-gen-1',
        text: "Robots will learn 'affordances' (graspable, pushable) applicable across objects/contexts, rather than thousands of specific tasks, enabling compositional skill generalization.",
        dateMade: new Date().toISOString(),
        topic: "Learning Affordances for Generalization"
      },
      {
        id: 'rh-pred-gen-2',
        text: "Large language models will be critical for scaffolding compositional skills in robots, decomposing natural language instructions (e.g., 'make coffee') into sequences of learned physical affordances.",
        dateMade: new Date().toISOString(),
        topic: "Language as Scaffolding for Skill Composition"
      }
    ],
    company: 'Google DeepMind',
    linkedin: 'https://linkedin.com/in/raia-hadsell-3151877',
    twitter: 'https://twitter.com/raiahadsell',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://deepmind.google/discover/people/raia-hadsell',
  },
  {
    id: '33',
    name: 'Pushmeet Kohli',
    title: 'Head of AI for Science, Google DeepMind',
    bio: 'Heads Google DeepMind\'s AI for Science initiative, applying AI to solve complex scientific problems, notably in biology (e.g., AlphaFold).',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Pushmeet Kohli',
    expertise: ['AI for Science', 'Machine Learning', 'Computer Vision', 'Computational Biology', 'AlphaFold'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [],
    company: 'Google DeepMind',
    linkedin: 'https://linkedin.com/in/pushmeet-kohli-b401211',
    twitter: 'https://twitter.com/pushmeet',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://deepmind.google/discover/people/pushmeet-kohli',
  },
  {
    id: '34',
    name: 'Samy Bengio',
    title: 'Senior Director of AI and ML Research, Apple',
    bio: 'An experienced AI researcher leading AI and ML research at Apple, formerly a distinguished scientist at Google.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Samy Bengio',
    expertise: ['Machine Learning', 'Deep Learning', 'AI Research', 'Computer Vision', 'Speech Recognition'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [],
    company: 'Apple (formerly Google)',
    linkedin: 'https://linkedin.com/in/samybengio',
    twitter: 'https://twitter.com/samybengio',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://samy.bengio.ch',
  },
  {
    id: '35',
    name: 'Zoubin Ghahramani',
    title: 'VP of Research, Google AI / Professor, University of Cambridge',
    bio: 'Leads research at Google AI and is a Professor at the University of Cambridge, with expertise in Bayesian machine learning and AI.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Zoubin Ghahramani',
    expertise: ['Machine Learning', 'Bayesian Methods', 'AI Research', 'Statistics', 'Probabilistic Models'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [],
    company: 'Google AI / University of Cambridge',
    linkedin: 'https://linkedin.com/in/zoubin-ghahramani-b36203',
    twitter: 'https://twitter.com/zoubin',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://mlg.eng.cam.ac.uk/zoubin',
  },
  {
    id: '36',
    name: 'Eric Horvitz',
    title: 'Chief Scientific Officer, Microsoft',
    bio: 'Microsoft\'s Chief Scientific Officer, with extensive work on AI principles, human-AI collaboration, and the societal impacts of AI.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Eric Horvitz',
    expertise: ['Artificial Intelligence', 'Human-AI Collaboration', 'AI Ethics', 'Decision Theory', 'AI Policy'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [],
    company: 'Microsoft',
    linkedin: 'https://linkedin.com/in/erichorvitz',
    twitter: 'https://twitter.com/erichorvitz',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://erichorvitz.com',
  },
  {
    id: '37',
    name: 'Xuedong Huang',
    title: 'Azure AI CTO, Microsoft Technical Fellow',
    bio: 'Microsoft Technical Fellow and CTO for Azure AI, a pioneer in speech recognition and AI platforms.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Xuedong Huang',
    expertise: ['Speech Recognition', 'AI Platforms', 'Machine Learning', 'Natural Language Processing', 'Deep Learning'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [],
    company: 'Microsoft',
    linkedin: 'https://linkedin.com/in/xuedong-huang-a77b312',
    twitter: 'https://twitter.com/xuedonghuang',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://www.microsoft.com/en-us/research/people/xdh',
  },
  {
    id: '38',
    name: 'Jianfeng Gao',
    title: 'Distinguished Scientist, VP, Microsoft Research',
    bio: 'Distinguished Scientist and VP at Microsoft Research, focusing on natural language processing, deep learning, and large language models.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Jianfeng Gao',
    expertise: ['Natural Language Processing', 'Deep Learning', 'Large Language Models', 'Machine Translation'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [],
    company: 'Microsoft Research',
    linkedin: 'https://linkedin.com/in/jianfeng-gao-9252321',
    twitter: 'https://twitter.com/jifengo',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://www.microsoft.com/en-us/research/people/jfgao',
  },
  {
    id: '39',
    name: 'Alex Smola',
    title: 'VP/Distinguished Scientist, Amazon Web Services / Professor, CMU',
    bio: 'VP and Distinguished Scientist at AWS, and Professor at CMU, with expertise in machine learning, deep learning, and scalable AI.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Alex Smola',
    expertise: ['Machine Learning', 'Deep Learning', 'Scalable AI', 'Optimization', 'Kernel Methods'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [],
    company: 'Amazon Web Services / Carnegie Mellon University',
    linkedin: 'https://linkedin.com/in/alexsmola',
    twitter: 'https://twitter.com/alexsmola',
    instagram: undefined,
    facebook: undefined,
    github: 'https://github.com/smola',
    website: 'https://alex.smola.org',
  },
  {
    id: '40',
    name: 'Manuela Veloso',
    title: 'Head of J.P. Morgan AI Research / Professor Emerita, CMU',
    bio: 'Leads AI Research at J.P. Morgan and is Professor Emerita at CMU, a pioneer in autonomous robots and symbiotic human-robot interaction.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Manuela Veloso',
    expertise: ['Robotics', 'Artificial Intelligence', 'Machine Learning', 'Human-Robot Interaction', 'AI Planning'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [],
    company: 'J.P. Morgan AI Research / Carnegie Mellon University',
    linkedin: 'https://linkedin.com/in/manuela-veloso-1823931',
    twitter: undefined,
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://www.cs.cmu.edu/~mmv',
  },
  {
    id: '41',
    name: 'Aidan Gomez',
    title: 'CEO, Cohere',
    bio: 'CEO of Cohere and a co-author of the influential "Attention Is All You Need" paper, which introduced the Transformer architecture.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Aidan Gomez',
    expertise: ['Natural Language Processing', 'Transformers', 'Large Language Models', 'AI Entrepreneurship', 'Deep Learning'],
    impactArea: 'Key Innovators & Entrepreneurs',
    predictions: [],
    company: 'Cohere',
    linkedin: 'https://linkedin.com/in/aidan-gomez',
    twitter: 'https://twitter.com/aidangomez',
    instagram: undefined,
    facebook: undefined,
    github: 'https://github.com/aidangomez',
    website: 'https://cohere.com/about',
  },
  {
    id: '42',
    name: 'Noam Shazeer',
    title: 'CEO, Character.ai',
    bio: 'CEO of Character.ai, co-author of "Attention Is All You Need," and key contributor to large-scale neural networks at Google.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Noam Shazeer',
    expertise: ['Natural Language Processing', 'Transformers', 'Large Language Models', 'AI Entrepreneurship', 'Large-Scale Systems'],
    impactArea: 'Key Innovators & Entrepreneurs',
    predictions: [],
    company: 'Character.ai (formerly Google)',
    linkedin: 'https://linkedin.com/in/noam-shazeer-2618a75',
    twitter: 'https://twitter.com/noamshazeer',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://character.ai',
  },
  {
    id: '43',
    name: 'Ashish Vaswani',
    title: 'CEO, Essential AI',
    bio: 'CEO of Essential AI and lead author of the "Attention Is All You Need" paper, fundamental to modern NLP. Formerly at Google.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Ashish Vaswani',
    expertise: ['Natural Language Processing', 'Transformers', 'Deep Learning', 'AI Entrepreneurship', 'Sequence Modeling'],
    impactArea: 'Key Innovators & Entrepreneurs',
    predictions: [],
    company: 'Essential AI (formerly Google)',
    linkedin: 'https://linkedin.com/in/ashish-vaswani-8b143a12',
    twitter: 'https://twitter.com/ashvaswani',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://adept.ai/team',
  },
  {
    id: '44',
    name: 'Jakob Uszkoreit',
    title: 'CEO, Inceptive',
    bio: 'CEO of Inceptive and co-author of "Attention Is All You Need." Formerly a key researcher at Google Brain.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Jakob Uszkoreit',
    expertise: ['Natural Language Processing', 'Transformers', 'Computational Biology', 'AI Entrepreneurship', 'Deep Learning'],
    impactArea: 'Key Innovators & Entrepreneurs',
    predictions: [],
    company: 'Inceptive (formerly Google)',
    linkedin: 'https://linkedin.com/in/jakob-uszkoreit-3407941',
    twitter: 'https://twitter.com/jacobush',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://inhuman.ai',
  },
  {
    id: '45',
    name: 'Llion Jones',
    title: 'Co-founder, Sakana AI',
    bio: 'Co-author of "Attention Is All You Need" while at Google, now co-founder of Sakana AI, focusing on new AI architectures.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Llion Jones',
    expertise: ['Natural Language Processing', 'Transformers', 'AI Architectures', 'Deep Learning'],
    impactArea: 'Key Innovators & Entrepreneurs',
    predictions: [],
    company: 'Sakana AI (formerly Google)',
    linkedin: 'https://linkedin.com/in/llion-jones-33298a2',
    twitter: 'https://twitter.com/llionjones',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: undefined,
  },
  {
    id: '46',
    name: 'Illia Polosukhin',
    title: 'Co-founder, NEAR Protocol',
    bio: 'Co-founder of NEAR Protocol and co-author of "Attention Is All You Need." Contributed to TensorFlow at Google.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Illia Polosukhin',
    expertise: ['Blockchain', 'Natural Language Processing', 'Transformers', 'Decentralized Systems', 'TensorFlow'],
    impactArea: 'Key Innovators & Entrepreneurs',
    predictions: [],
    company: 'NEAR Protocol (formerly Google)',
    linkedin: 'https://linkedin.com/in/illia-polosukhin-3064391a',
    twitter: 'https://twitter.com/ilblackdragon',
    instagram: undefined,
    facebook: undefined,
    github: 'https://github.com/ilblackdragon',
    website: 'https://near.org/illia-polosukhin',
  },
  {
    id: '47',
    name: 'Niki Parmar',
    title: 'Co-founder, Essential AI',
    bio: 'Co-founder of Essential AI and co-author of "Attention Is All You Need." Formerly a researcher at Google Brain.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Niki Parmar',
    expertise: ['Natural Language Processing', 'Transformers', 'Deep Learning', 'AI Entrepreneurship', 'Computer Vision'],
    impactArea: 'Key Innovators & Entrepreneurs',
    predictions: [],
    company: 'Essential AI (formerly Google)',
    linkedin: 'https://linkedin.com/in/nikiparmar',
    twitter: 'https://twitter.com/nikiparmar_',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://adept.ai/team',
  },
  {
    id: '48',
    name: 'Lukasz Kaiser',
    title: 'Researcher, OpenAI',
    bio: 'Co-author of "Attention Is All You Need" and the Tensor2Tensor library while at Google. Now a researcher at OpenAI.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Lukasz Kaiser',
    expertise: ['Deep Learning', 'Natural Language Processing', 'Transformers', 'Machine Learning Systems', 'TensorFlow'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [],
    company: 'OpenAI (formerly Google)',
    linkedin: 'https://linkedin.com/in/lukasz-kaiser-2a8183',
    twitter: 'https://twitter.com/lukaszkaiser',
    instagram: undefined,
    facebook: undefined,
    github: 'https://github.com/lukaszkaiser',
    website: undefined,
  },
  {
    id: '49',
    name: 'David Silver',
    title: 'Principal Research Scientist, Google DeepMind',
    bio: 'Led the AlphaGo project, which defeated a world champion Go player, and subsequent projects like AlphaZero and AlphaStar at Google DeepMind.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'David Silver',
    expertise: ['Reinforcement Learning', 'Deep Learning', 'Game AI', 'AGI Research', 'AlphaGo', 'AlphaZero'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [],
    company: 'Google DeepMind',
    linkedin: undefined,
    twitter: undefined,
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://deepmind.google/discover/people/david-silver',
  },
  {
    id: '50',
    name: 'Vladlen Koltun',
    title: 'Distinguished Scientist, Apple',
    bio: 'A distinguished scientist at Apple, previously at Intel Labs, known for contributions to computer vision, deep learning, and robotics.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Vladlen Koltun',
    expertise: ['Computer Vision', 'Deep Learning', 'Robotics', '3D Reconstruction', 'Simulation'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [],
    company: 'Apple (formerly Intel Labs)',
    linkedin: 'https://linkedin.com/in/vladlen-koltun-4139a04',
    twitter: 'https://twitter.com/vladlenkoltun',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://vladlen.info',
  },
  {
    id: '51',
    name: 'Andrew Ng',
    title: 'Founder, Landing AI / General Partner, AI Fund / Co-founder, Coursera / Adjunct Professor, Stanford',
    bio: 'A leading figure in deep learning and online education. Co-founded Coursera and Google Brain. Founder of Landing AI and AI Fund.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Andrew Ng',
    expertise: ['Deep Learning', 'Machine Learning', 'AI Education', 'AI Strategy', 'Online Education'],
    impactArea: 'Influential Academics',
    predictions:[
          {
              "id": "an-pred-cat1-1",
              "text": "The focus on AGI 'extinction risk' is a profound and harmful distraction. We must focus on the real, present-day challenges of AI: bias, fairness, job displacement, and equitable access. Worrying about a hypothetical evil superintelligence is taking oxygen away from solving problems that affect millions of people today.",
              "dateMade": "2024-05-21T12:00:00.000Z",
              "topic": "Risk Stance"
          },
          {
              "id": "an-pred-cat1-2",
              "text": "The greatest risk associated with AI is the opportunity cost of *not* developing and deploying it. Failing to apply AI to urgent problems in healthcare, climate change, and education is a tangible harm we inflict on society through excessive caution and fear-mongering.",
              "dateMade": "2024-05-21T12:00:00.000Z",
              "topic": "Risk Stance"
          },
          {
              "id": "an-pred-cat2-1",
              "text": "AI is the new electricity. It is a general-purpose technology that will transform every industry. The ultimate value, however, will not be in the core technology itself but in the countless applications built on top of it. The conversation must shift from algorithms to applications.",
              "dateMade": "2024-05-21T12:00:00.000Z",
              "topic": "AI's Societal Impact"
          },
          {
              "id": "an-pred-cat2-2",
              "text": "The primary solution to job displacement from AI is not UBI, but education and continuous reskilling. We must build a society of lifelong learners and empower individuals with the skills to work *with* AI tools, which will augment human potential and create new, unforeseen jobs.",
              "dateMade": "2024-05-21T12:00:00.000Z",
              "topic": "AI's Societal Impact"
          },
          {
              "id": "an-pred-cat3-1",
              "text": "The single most important shift in building practical AI systems is the move from a model-centric to a data-centric approach. For the vast majority of real-world applications, the bottleneck to performance is the quality of the data, not the novelty of the algorithm. Systematically engineering your data is the key.",
              "dateMade": "2024-05-21T12:00:00.000Z",
              "topic": "Technical Vision"
          },
          {
              "id": "an-pred-cat3-2",
              "text": "While large foundation models are impressive, much of the future economic value of AI will be captured by smaller, more efficient models that are fine-tuned on high-quality, proprietary data for specific tasks. Not every business problem needs a 100-billion-parameter model.",
              "dateMade": "2024-05-21T12:00:00.000Z",
              "topic": "Technical Vision"
          },
          {
              "id": "an-pred-cat4-1",
              "text": "Regulation should be vertical (application-specific), not horizontal (technology-specific). We should regulate AI in self-driving cars and AI in healthcare differently, based on their unique risks. A broad, horizontal law regulating 'AI' or 'foundation models' would be a terrible idea that stifles innovation.",
              "dateMade": "2024-05-21T12:00:00.000Z",
              "topic": "Governance & Action"
          },
          {
              "id": "an-pred-cat4-2",
              "text": "The most powerful action we can take to ensure a positive future with AI is to democratize knowledge. Widespread AI literacy, from the C-suite to the factory floor, is the prerequisite for both responsible innovation and sensible policymaking. Education is the antidote to fear.",
              "dateMade": "2024-05-21T12:00:00.000Z",
              "topic": "Governance & Action"
          },
          {
              "id": "an-pred-cat4-3",
              "text": "Calls to 'pause' AI development are profoundly misguided. Progress on AI is essential for our economic competitiveness and for solving humanity's grand challenges. Hitting pause only benefits those who are already ahead and slows our ability to find solutions.",
              "dateMade": "2024-05-21T12:00:00.000Z",
              "topic": "Governance & Action"
          }
      ],
    company: 'Landing AI / AI Fund / Coursera / Stanford University',
    linkedin: 'https://linkedin.com/in/andrewyng',
    twitter: 'https://twitter.com/andrewyng',
    instagram: 'https://instagram.com/andrewyng',
    facebook: 'https://facebook.com/andrew.ng.96',
    github: undefined,
    website: 'https://www.andrewng.org',
  },
  {
    id: '52',
    name: 'Pieter Abbeel',
    title: 'Professor, UC Berkeley / Co-founder, Covariant',
    bio: 'Professor at UC Berkeley and co-founder of Covariant, specializing in robotics, reinforcement learning, and deep learning for robot control.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Pieter Abbeel',
    expertise: ['Robotics', 'Reinforcement Learning', 'Deep Learning', 'AI Entrepreneurship', 'Robot Learning'],
    impactArea: 'Influential Academics',
    predictions: [
      {
          "id": "yb-pred-pa1-1",
          "text": "The Primary Risks are Practical, Not Existential: The immediate and critical challenges for AI are in the physical world: ensuring robot safety around humans, guaranteeing system reliability in critical infrastructure like supply chains, and managing the economic impact of automation. Debates on hypothetical superintelligence distract from these tangible, near-term problems.",
          "dateMade": "2024-05-21T10:00:00.000Z",
          "topic": "Risk Stance"
      },
      {
          "id": "yb-pred-pa1-2",
          "text": "AI as an Augmentation Engine: AI's greatest potential lies in augmenting human labor, not replacing it. The goal is to build collaborative systems where robots handle the dull, dirty, and dangerous tasks, freeing humans for higher-level problem-solving, creativity, and oversight.",
          "dateMade": "2024-05-21T10:00:00.000Z",
          "topic": "Risk Stance"
      },
      {
          "id": "yb-pred-pa2-1",
          "text": "The 'GPT Moment' for Robotics is Coming: The path to general-purpose robots is through creating a single 'foundation model' for robotics. By training on vast, diverse datasets of physical interaction from millions of robots, we can build a single AI brain that enables robots to generalize to novel tasks and environments with minimal fine-tuning.",
          "dateMade": "2024-05-21T10:00:00.000Z",
          "topic": "Technical Vision"
      },
      {
          "id": "yb-pred-pa2-2",
          "text": "Intelligence Must Be Embodied: True common sense and a deep understanding of causality will not emerge from text or images alone; they must be learned through physical interaction with the world. Embodiment is a non-negotiable prerequisite for reaching the next level of artificial intelligence.",
          "dateMade": "2024-05-21T10:00:00.000Z",
          "topic": "Technical Vision"
      },
      {
          "id": "yb-pred-pa2-3",
          "text": "Simulation is the Scaffolding for Real-World AI: The bottleneck to robotic learning is collecting real-world data. The future of robotics will be dominated by learning in high-fidelity simulations at massive scale, making the 'sim-to-real' transfer the single most critical research problem to solve for building capable, real-world AI systems.",
          "dateMade": "2024-05-21T10:00:00.000Z",
          "topic": "Technical Vision"
      },
      {
          "id": "yb-pred-pa3-1",
          "text": "Accelerate, Don't Pause: Calls to pause AI development are misguided. The world faces immense challenges in supply chains, labor shortages, and productivity that advanced robotics can solve. We should accelerate research and deployment, while focusing engineering rigor on building robust and safe systems.",
          "dateMade": "2024-05-21T10:00:00.000Z",
          "topic": "Governance & Action"
      },
      {
          "id": "yb-pred-pa3-2",
          "text": "Industry-Led Standards are the Path to Safety: The most effective path to safe and trustworthy robotics is through industry-led standards, rigorous testing protocols, and performance benchmarks. The experts building and deploying these systems are best positioned to define what 'safe' and 'reliable' mean in practice, rather than broad, top-down regulation that stifles innovation.",
          "dateMade": "2024-05-21T10:00:00.000Z",
          "topic": "Governance & Action"
      }
  ],
    company: 'UC Berkeley / Covariant',
    linkedin: 'https://linkedin.com/in/pieter-abbeel-4067a51',
    twitter: 'https://twitter.com/pieterabbeel',
    instagram: undefined,
    facebook: undefined,
    github: 'https://github.com/pieterabbeel',
    website: 'https://people.eecs.berkeley.edu/~pabbeel',
  },
  {
    id: '53',
    name: 'Anima Anandkumar',
    title: 'Bren Professor, Caltech / Sr. Director of AI Research, NVIDIA',
    bio: 'Bren Professor at Caltech and Senior Director of AI Research at NVIDIA, working on tensor methods, deep learning, and large-scale machine learning.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Anima Anandkumar',
    expertise: ['Machine Learning', 'Deep Learning', 'Tensor Methods', 'High-Performance Computing', 'AI for Science'],
    impactArea: 'Influential Academics',
    predictions: [{
      "id": "yb-pred-aa1-1",
      "text": "The Gravest Risk is Centralization, Not Skynet: The most pressing danger is the concentration of power in a few corporations building closed, proprietary AI models. This creates a 'single point of failure' for society, stifles open scientific inquiry, and prevents democratic oversight. The antidote is a radical commitment to open-source models and decentralized research.",
      "dateMade": "2024-05-21T10:05:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-aa1-2",
      "text": "The Real-World Risk is Unreliable Science and Engineering: The immediate threat is not a rogue superintelligence but an AI system hallucinating a flawed protein structure or miscalculating the fluid dynamics for a new aircraft. Over-reliance on black-box models without rigorous validation, uncertainty quantification, and physical grounding is a recipe for engineering disasters and scientific setbacks.",
      "dateMade": "2024-05-21T10:05:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-aa2-1",
      "text": "The Next Frontier is AI for Science, Grounded in First Principles: The current paradigm of scaling data and compute will hit a wall. The true revolution will come from integrating the laws of physics, chemistry, and biology directly into AI architectures. Physics-Informed AI, like Neural Operators for solving differential equations, will achieve far greater generalization and data efficiency than today's purely data-driven models, unlocking breakthroughs in climate modeling, drug discovery, and fusion energy.",
      "dateMade": "2024-05-21T10:05:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-aa2-2",
      "text": "We Must Move Beyond the Transformer Monoculture: The Transformer, while historically important, is not the ultimate architecture. Its quadratic complexity is a fundamental bottleneck, and it's poorly suited for many scientific domains. The future requires a new class of foundation models, such as those based on operator theory and neural PDEs, that are more computationally efficient and inherently structured to understand complex physical systems.",
      "dateMade": "2024-05-21T10:05:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-aa2-3",
      "text": "Rigor and Mathematical Foundations are Non-Negotiable: 'Scaling is all you need' is an incomplete and dangerous philosophy. Sustainable progress requires a return to mathematical rigor. We need to build AI on solid theoretical ground using tools like tensor methods and operator theory to create models that are not just performant but also interpretable, robust, and provably stable.",
      "dateMade": "2024-05-21T10:05:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-aa3-1",
      "text": "Champion and Fund an Open Scientific AI Ecosystem: Governments and academic institutions must actively counter the privatization of AI. This means mandating that publicly funded research uses and produces open-source models, investing in public compute infrastructure like the National AI Research Resource (NAIRR), and creating standards for model transparency and reproducibility.",
      "dateMade": "2024-05-21T10:05:00.000Z",
      "topic": "Governance & Action"
      },
      {
      "id": "yb-pred-aa3-2",
      "text": "Algorithm-Hardware Co-Design is the Engine of Progress: Future breakthroughs won't come from algorithms or hardware in isolation, but from their synthesis. We need to co-design next-generation AI models (like Fourier Neural Operators) and the specialized hardware (GPUs and beyond) that can run them efficiently. This symbiotic relationship is the critical path to solving grand scientific challenges.",
      "dateMade": "2024-05-21T10:05:00.000Z",
      "topic": "Governance & Action"
      }],
    company: 'Caltech / NVIDIA',
    linkedin: 'https://linkedin.com/in/anima-anandkumar-16b7391b',
    twitter: 'https://twitter.com/anima_anand',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://tensorlab.cms.caltech.edu/users/anima',
  },
  {
    id: '54',
    name: 'Percy Liang',
    title: 'Associate Professor, Stanford HAI',
    bio: 'Associate Professor at Stanford and affiliated with HAI, focusing on natural language processing, trustworthy AI, and machine learning.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Percy Liang',
    expertise: ['Natural Language Processing', 'Trustworthy AI', 'Machine Learning', 'AI Ethics', 'Foundation Models'],
    impactArea: 'Influential Academics',
    predictions: [{
      "id": "yb-pred-pl1-1",
      "text": "The Core Risk is the 'Trust Gap' in a Flawed Ecosystem: The most urgent threat is not hypothetical future AI but the unreliability of current systems. We are deploying powerful, inscrutable models into critical societal roles without a rigorous science to understand their failure modes. The risk is systemic: a brittle, untrustworthy infrastructure built on models whose behavior we can't fully predict or control, leading to harms from bias, misinformation, and cascading failures.",
      "dateMade": "2024-05-21T10:10:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-pl1-2",
      "text": "Centralization is a Primary Vector for Harm: The concentration of foundation model development within a few large tech companies creates immense societal risk. It leads to a monoculture of values, a lack of independent auditing, and an accountability vacuum. This centralization of power is a more immediate and concrete threat to democracy and equity than speculative 'runaway AI' scenarios.",
      "dateMade": "2024-05-21T10:10:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-pl2-1",
      "text": "The Future is Holistic, Standardized Evaluation: 'Better than human' on a narrow benchmark is a meaningless and dangerous metric. The most critical technical work is creating comprehensive, standardized evaluation platforms (a 'Consumer Reports' for AI). We must systematically measure models across dozens of vectors: robustness, fairness, calibration, reasoning, toxicity, efficiency, and factuality. This is the only path to building trustworthy systems.",
      "dateMade": "2024-05-21T10:10:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-pl2-2",
      "text": "We Need a Science of Foundation Models, Not Just Engineering: We are currently in an era of AI alchemy, scaling up models without a deep, predictive understanding of why they work or how they will fail. The central research challenge is to move from phenomenology to a genuine science of these artifacts. This involves developing theories to explain emergent properties and creating principles for building models that are interpretable and reliable by design.",
      "dateMade": "2024-05-21T10:10:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-pl2-3",
      "text": "Move Beyond Static Benchmarks to Interactive Grounding: True language understanding cannot be measured by static question-answering datasets. The next generation of NLP models must be interactive and grounded. They must be able to ask clarifying questions, state their uncertainties, and update their understanding based on dialogue and feedback from the real world, moving from mere pattern matching to collaborative sense-making.",
      "dateMade": "2024-05-21T10:10:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-pl3-1",
      "text": "Mandate Radical Transparency and Independent Auditing: The single most effective action we can take is to legislate transparency. This means requiring companies to publish detailed 'datasheets' for their training data and 'model cards' for their models. Crucially, it also means mandating API access for qualified, independent researchers to enable deep, adversarial auditing of public-facing models to uncover societal harms.",
      "dateMade": "2024-05-21T10:10:00.000Z",
      "topic": "Governance & Action"
      },
      {
      "id": "yb-pred-pl3-2",
      "text": "Govern Through Measurement, Not Abstract Principles: Vague ethical principles are insufficient. Effective governance and regulation must be anchored in concrete, measurable standards. By establishing and promoting standard evaluation suites for trustworthiness, we can create a clear basis for policy, allowing regulators to set specific, auditable thresholds that AI systems must meet before they can be deployed in high-stakes domains.",
      "dateMade": "2024-05-21T10:10:00.000Z",
      "topic": "Governance & Action"
      }],
    company: 'Stanford HAI',
    linkedin: 'https://linkedin.com/in/percy-liang-2a93902',
    twitter: 'https://twitter.com/percyliang',
    instagram: undefined,
    facebook: undefined,
    github: 'https://github.com/percyliang',
    website: 'https://cs.stanford.edu/~pliang',
  },
  {
    id: '55',
    name: 'Chris Manning',
    title: 'Professor of Linguistics and Computer Science, Stanford',
    bio: 'Leading researcher in natural language processing at Stanford, known for deep learning approaches to NLP and co-author of influential textbooks.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Chris Manning',
    expertise: ['Natural Language Processing', 'Deep Learning', 'Computational Linguistics', 'Sentiment Analysis'],
    impactArea: 'Influential Academics',
    predictions: [{
      "id": "yb-pred-cm1-1",
      "text": "The Risk is Our Misunderstanding: The primary danger is not that models will become sentient, but that we fundamentally misunderstand what they are. We anthropomorphize sophisticated pattern-matching machines, attributing 'beliefs' and 'understanding' to them. This leads to misuse, over-trust, and societal disruption when these brittle, non-reasoning systems inevitably fail in consequential ways.",
      "dateMade": "2024-05-21T10:15:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-cm1-2",
      "text": "Existential Risk is a Distraction from Imminent Harms: The focus on long-term, speculative risks of superintelligence is a harmful distraction from the very real, present-day harms of LLMs: amplifying bias, generating misinformation at scale, and degrading the information ecosystem. We have a five-alarm fire of societal problems right now; we should be fighting that, not worrying about a hypothetical meteor.",
      "dateMade": "2024-05-21T10:15:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-cm2-1",
      "text": "The Future Requires Compositional, Structured Models: Scaling up unstructured Transformers has been incredibly effective, but it's a dead end for achieving true reasoning. The next breakthrough will come from models that incorporate explicit structure, drawing on insights from linguistics and computer science to achieve genuine compositional understanding—the ability to build complex meanings from simpler parts, a core property of human language that current LLMs lack.",
      "dateMade": "2024-05-21T10:15:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-cm2-2",
      "text": "We Need Meaning, Not Just Form: Current LLMs are masters of linguistic form but have no access to meaning (semantics). They don't know what words refer to in the real world. The central challenge for NLP is to build models that are grounded—connecting language to perception, action, and a structured world model. Without this grounding, we will never achieve true language understanding.",
      "dateMade": "2024-05-21T10:15:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-cm2-3",
      "text": "Linguistics is More Important Than Ever: The 'bitter lesson'—that general methods leveraged with computation are all that matter—is incomplete. While scale is powerful, a deep understanding of the principles of human language from the field of linguistics is not optional; it's essential for identifying the shortcomings of our current models and designing the next generation of architectures that can actually reason and generalize systematically.",
      "dateMade": "2024-05-21T10:15:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-cm3-1",
      "text": "Focus on Scientific Evaluation, Not Corporate Demos: Progress is being distorted by marketing hype and leaderboard-chasing. The most critical action for the community is to develop and commit to rigorous, scientifically-valid evaluation suites that test for reasoning, robustness, and factual accuracy, rather than superficial fluency. We need to create a culture of honest assessment, not a race to impress.",
      "dateMade": "2024-05-21T10:15:00.000Z",
      "topic": "Governance & Action"
      },
      {
      "id": "yb-pred-cm3-2",
      "text": "Invest in Education and Foundational Research: The best way to ensure responsible AI development is to create a generation of scientists and engineers who deeply understand the technology's foundations and limitations. Instead of focusing solely on top-down regulation, we should massively increase funding for public, university-led fundamental research and education in AI and NLP, creating a counterbalance to corporate narratives and a source of independent expertise.",
      "dateMade": "2024-05-21T10:15:00.000Z",
      "topic": "Governance & Action"
      }],
    company: 'Stanford University',
    linkedin: 'https://linkedin.com/in/christopher-manning-17a23b5',
    twitter: 'https://twitter.com/chrmanning',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://nlp.stanford.edu/manning',
  },
  {
    id: '56',
    name: 'Daniela Rus',
    title: 'Director, MIT CSAIL / Professor, MIT',
    bio: 'Director of MIT CSAIL and Professor at MIT, a prominent researcher in robotics, mobile computing, and distributed systems.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Daniela Rus',
    expertise: ['Robotics', 'Mobile Computing', 'Distributed Systems', 'AI Leadership', 'Networked Systems'],
    impactArea: 'Influential Academics',
    predictions: [{
      "id": "yb-pred-dr1-1",
      "text": "The Risk is Brittle Systems, Not Malicious Ones: The most significant and immediate danger is the deployment of physically embodied AI systems—robots, drones, autonomous cars—that are not robust enough for the chaotic real world. The risk isn't that a robot will want to harm us, but that it will fail in an unexpected way with physical consequences, like in a hospital, on a factory floor, or on our roads.",
      "dateMade": "2024-05-21T10:20:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-dr1-2",
      "text": "The Risk of an 'AI Access Divide': A critical societal risk is that the benefits of AI and robotics—in healthcare, labor, and convenience—will only accrue to the wealthy and the technologically advanced. Without intentional design and policy, we risk creating a world with a profound divide between those who have access to robotic augmentation and those who do not.",
      "dateMade": "2024-05-21T10:20:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-dr2-1",
      "text": "The Future is Pervasive, Embodied Intelligence: We are moving from an era of computation in the cloud to an era of 'physical AI' all around us. The next great leap is not bigger LLMs, but intelligence embedded in everything: soft robots that can assist the elderly, smart materials that can self-repair, and swarms of autonomous vehicles that coordinate to solve complex logistical problems. The goal is a world infused with helpful computation.",
      "dateMade": "2024-05-21T10:20:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-dr2-2",
      "text": "The Soft Robotics Revolution: The key to integrating robots into human-centric environments is to move beyond rigid, metal machines. The future belongs to soft, compliant, bio-inspired robots. These machines, made of flexible materials, will be inherently safer, more adaptable, and better suited for tasks that require a gentle touch, from agriculture to assisted living and minimally invasive surgery.",
      "dateMade": "2024-05-21T10:20:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-dr2-3",
      "text": "Democratizing Robot Creation with 'Compilers for Robots': A fundamental barrier to progress is that building robots is too hard. The vision is to create platforms that allow anyone to design, simulate, fabricate, and program their own custom robots for specific tasks, without being a robotics expert. This 'democratization of robotics' will unleash a Cambrian explosion of innovation across countless fields.",
      "dateMade": "2024-05-21T10:20:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-dr3-1",
      "text": "We Need a 'Driver's License' for Autonomous Systems: Before deploying a high-stakes AI system into the world, we must have a rigorous, standardized process for certifying its safety and competence. This 'driver's license' would involve a battery of tests in both simulation and real-world testbeds to ensure the system is reliable, robust, and safe before it can operate around people.",
      "dateMade": "2024-05-21T10:20:00.000Z",
      "topic": "Governance & Action"
      },
      {
      "id": "yb-pred-dr3-2",
      "text": "Proactive Governance to Steer AI Towards Societal Good: Instead of just regulating to prevent harms, we must proactively steer AI innovation toward solving humanity's grand challenges. This involves massive public and private investment in using AI and robotics to tackle problems like climate change, accessible healthcare, and sustainable agriculture, creating a positive vision that guides the field's development.",
      "dateMade": "2024-05-21T10:20:00.000Z",
      "topic": "Governance & Action"
      }],
    company: 'MIT CSAIL',
    linkedin: 'https://linkedin.com/in/daniela-rus-6b040b6',
    twitter: 'https://twitter.com/danielarus',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://danielarus.csail.mit.edu',
  },
  {
    id: '57',
    name: 'Yejin Choi',
    title: 'Professor, University of Washington / Senior Research Director, AI2',
    bio: 'Professor at the University of Washington and Senior Research Director at AI2. MacArthur Fellow known for work in NLP, commonsense reasoning, and AI ethics.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Yejin Choi',
    expertise: ['Natural Language Processing', 'Commonsense Reasoning', 'AI Ethics', 'Computational Linguistics', 'Neuro-Symbolic AI'],
    impactArea: 'Influential Academics',
    predictions: [{
      "id": "yb-pred-yc1-1",
      "text": "The Risk is 'Amoral Intelligence', Not 'Superintelligence': The primary danger is not a future AI that becomes smarter than us, but the current reality of deploying systems with high linguistic fluency but zero moral or social commonsense. These systems are like 'sociopaths in a box'—they can generate persuasive text but have no understanding of harm, social norms, or ethics. This leads to immediate, real-world damage through toxic content, harmful advice, and the erosion of shared values.",
      "dateMade": "2024-05-21T10:25:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-yc1-2",
      "text": "Focusing on Extinction is a Category Error that Obscures Real Threats: Debating AI-induced human extinction is a profound misunderstanding of what these models are. They are not agents with desires; they are complex statistical systems. This sci-fi narrative distracts from the tangible, urgent risks: building models that are racist, sexist, and classist because they are trained on our own biased data, creating a world where AI perpetuates and amplifies the worst of human society.",
      "dateMade": "2024-05-21T10:25:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-yc2-1",
      "text": "The Next Frontier is Modeling the 'Dark Matter' of Human Knowledge: Language is just the tip of the iceberg. True intelligence requires understanding the vast, unwritten 'dark matter' of commonsense—the intuitive knowledge about physics, social interactions, motivations, and causality that humans use effortlessly. The central technical challenge is to move beyond predicting text to building models that explicitly represent and reason over this implicit knowledge.",
      "dateMade": "2024-05-21T10:25:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-yc2-2",
      "text": "LLMs Are Not the End Goal; They Are a Component: We are too focused on the monolithic LLM. The future lies in a neuro-symbolic architecture where LLMs act as a powerful component—a 'fuzzy' knowledge extractor and natural language interface—that operates on and interacts with structured, symbolic knowledge graphs. This combination is the only way to achieve robust reasoning, planning, and genuine understanding that isn't brittle.",
      "dateMade": "2024-05-21T10:25:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-yc2-3",
      "text": "We Need to Teach AI Morality Like a Child, Not Program It Like a Computer: The 'alignment' problem won't be solved with a single clever objective function. Like a child learning right from wrong through countless examples, AI needs to learn social and moral norms from vast repositories of human judgments and stories. The future of AI safety is an empirical, data-driven effort to distill human values from the ground up, not a top-down philosophical exercise.",
      "dateMade": "2024-05-21T10:25:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-yc3-1",
      "text": "Mandate 'Commonsense Stress Tests' Before Deployment: We need to move beyond standard NLP benchmarks and create mandatory, public 'social and safety stress tests' for any powerful AI model. These audits must explicitly test for basic commonsense (e.g., 'Is it safe to mix ammonia and bleach?'), social reasoning, and adherence to fundamental moral principles before a model can be released to the public.",
      "dateMade": "2024-05-21T10:25:00.000Z",
      "topic": "Governance & Action"
      },
      {
      "id": "yb-pred-yc3-2",
      "text": "Democratize AI Safety Research by Creating Open, Shared Knowledge Resources: The antidote to biased, amoral AI from a few corporate labs is to create massive, open, public knowledge bases of commonsense and ethical norms (like the ATOMIC and Delphi projects). Funding and supporting these public-good resources is the most effective way to enable the entire global research community to build safer, more aligned AI.",
      "dateMade": "2024-05-21T10:25:00.000Z",
      "topic": "Governance & Action"
      }],
    company: 'University of Washington / Allen Institute for AI (AI2)',
    linkedin: 'https://linkedin.com/in/yejin-choi-52a1223',
    twitter: 'https://twitter.com/yejinchoi_ai',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://homes.cs.washington.edu/~yejin',
  },
  {
    id: '58',
    name: 'Chelsea Finn',
    title: 'Assistant Professor, Stanford University',
    bio: 'Assistant Professor at Stanford, researching meta-learning, reinforcement learning, and robotics, enabling robots to quickly learn new skills.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Chelsea Finn',
    expertise: ['Meta-Learning', 'Reinforcement Learning', 'Robotics', 'Machine Learning', 'Deep Learning'],
    impactArea: 'Influential Academics',
    predictions: [{
      "id": "yb-pred-cf1-1",
      "text": "The Risk is Brittle Generalization: The most insidious danger in robotics is deploying an agent that seems highly adaptable but fails catastrophically when it encounters a situation truly outside its 'meta-training' distribution. The risk is not a lack of intelligence, but a brittle illusion of adaptability, leading to unpredictable physical actions in novel, high-stakes environments like homes and hospitals.",
      "dateMade": "2024-05-21T10:30:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-cf1-2",
      "text": "The Meta-Learning Bias Trap: A critical risk is that the very ability to learn quickly becomes a vector for bias. If we train an agent to be adaptable using a distribution of tasks primarily from one culture or socioeconomic setting (e.g., tasks in wealthy, Western households), we create a system that is inherently biased, unable to generalize or adapt effectively in other contexts. This entrenches inequality at the foundational level of learning itself.",
      "dateMade": "2024-05-21T10:30:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-cf2-1",
      "text": "The Future is Learning to Learn, Not Just Learning: The path to general-purpose robots does not lie in building one giant model trained on every possible task. It lies in meta-learning—creating agents that learn the process of learning itself. The goal is an AI that, after meta-training on thousands of diverse tasks, can master a completely new one with only a handful of examples, mirroring the rapid adaptability of humans.",
      "dateMade": "2024-05-21T10:30:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-cf2-2",
      "text": "Unlocking Intelligence from Passive, In-the-Wild Data: The bottleneck for robotics is not algorithms, but diverse data. The next major breakthrough will come from models that can meta-learn skills and physical common sense from vast quantities of passive, unstructured video data (e.g., YouTube, human-perspective videos). This allows us to bootstrap robot learning without needing millions of hours of expensive and dangerous real-world robot interaction.",
      "dateMade": "2024-05-21T10:30:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-cf2-3",
      "text": "Offline RL + Meta-Learning is the Key to Safe Generalization: The future of safe and scalable robot learning is a two-stage process. First, use offline reinforcement learning to digest a large, static dataset of past experiences to learn a powerful, general world model and a set of basic skills. Second, use meta-learning on top of this foundation to enable rapid, safe, few-shot adaptation to new tasks, minimizing risky 'online' exploration in the real world.",
      "dateMade": "2024-05-21T10:30:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-cf3-1",
      "text": "Build the 'ImageNet for Generalization': The most critical action to accelerate progress and ensure democratic access is to create large-scale, open-source, diverse datasets of robotic interaction. We need a public 'meta-dataset' containing thousands of tasks across varied environments and robot morphologies. This is the essential public infrastructure required to train and benchmark truly generalist agents.",
      "dateMade": "2024-05-21T10:30:00.000Z",
      "topic": "Governance & Action"
      },
      {
      "id": "yb-pred-cf3-2",
      "text": "Develop 'Adaptability Standards' for Certification: We must shift our evaluation paradigm. Instead of certifying a robot on a fixed set of tasks, we need to certify its ability to adapt. We need standardized benchmarks that measure how quickly, robustly, and safely a system can learn and solve a new task from a held-out distribution. This should be the basis for safety certification.",
      "dateMade": "2024-05-21T10:30:00.000Z",
      "topic": "Governance & Action"
      }],
    company: 'Stanford University',
    linkedin: 'https://linkedin.com/in/chelsea-finn',
    twitter: 'https://twitter.com/chelseabfinn',
    instagram: undefined,
    facebook: undefined,
    github: 'https://github.com/cbfinn',
    website: 'https://ai.stanford.edu/~cbfinn',
  },
  {
    id: '59',
    name: 'Raquel Urtasun',
    title: 'Professor, University of Toronto / CEO, Waabi',
    bio: 'Professor at the University of Toronto and CEO of Waabi, focusing on machine perception for self-driving vehicles.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Raquel Urtasun',
    expertise: ['Computer Vision', 'Self-Driving Cars', 'Machine Learning', 'AI Entrepreneurship', '3D Perception'],
    impactArea: 'Influential Academics',
    predictions: [{
      "id": "yb-pred-ru1-1",
      "text": "The Real Risk is the 'Long Tail,' Not Malice: The critical threat in self-driving isn't rogue AI, but the astronomical number of rare 'edge cases' in the real world. A system that is 99.999% safe can still be deadly. The risk is deploying a system that is insufficiently tested against this long tail of unpredictable events, leading to accidents that erode public trust and set the industry back years.",
      "dateMade": "2024-05-21T10:35:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-ru1-2",
      "text": "The Risk of Brittle, Modular Systems: The traditional approach of stitching together hand-engineered modules for perception, prediction, and planning is a primary source of risk. It's brittle by nature; small errors in one module can cascade into catastrophic failures in another. The risk is clinging to this old paradigm instead of embracing a more robust, holistic approach.",
      "dateMade": "2024-05-21T10:35:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-ru2-1",
      "text": "The Future is One End-to-End AI Brain: The only path to building a truly scalable and robust self-driving system is to abandon the complex modular pipeline and replace it with a single, end-to-end AI model. This AI 'brain' learns to drive directly from sensor inputs, allowing it to discover complex relationships and handle novel situations in a way that hand-coded rules never could.",
      "dateMade": "2024-05-21T10:35:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-ru2-2",
      "text": "Simulation is the Only Path to Safety at Scale: We can never drive enough real-world miles to test for all possible edge cases. The solution is a high-fidelity, closed-loop simulator that acts as the primary training and testing ground. Real-world driving becomes a final validation step, not the main classroom. This 'simulation-first' approach is the only way to build a commercially viable and demonstrably safe system.",
      "dateMade": "2024-05-21T10:35:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-ru2-3",
      "text": "AI Must Generate the Test: To make simulation effective, we need an AI that automatically generates the most challenging and revealing scenarios to test the main driving AI. This creates an adversarial feedback loop where a 'world-building AI' constantly seeks out the weaknesses of the 'driving AI,' allowing us to systematically find and eliminate failure modes before a single truck hits the road.",
      "dateMade": "2024-05-21T10:35:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-ru3-1",
      "text": "Adopt Performance-Based Regulation, Not Prescriptive Rules: Governments should not dictate which sensors or software architectures to use. Instead, they must establish clear, technology-agnostic safety and performance benchmarks (e.g., 'must be X times safer than an average human driver'). This allows for rapid innovation while holding companies accountable to a high, measurable standard of safety.",
      "dateMade": "2024-05-21T10:35:00.000Z",
      "topic": "Governance & Action"
      },
      {
      "id": "yb-pred-ru3-2",
      "text": "Make High-Fidelity Simulation the Standard for Certification: The future of AV regulation and safety assurance is simulation. We need to develop and standardize transparent, verifiable simulation-based testing suites that can act as a 'virtual driving test' for AVs. This is the only scalable and economically feasible way to certify that an AI system is safe enough for public roads.",
      "dateMade": "2024-05-21T10:35:00.000Z",
      "topic": "Governance & Action"
      }],
    company: 'University of Toronto / Waabi',
    linkedin: 'https://linkedin.com/in/raquelurtasun',
    twitter: 'https://twitter.com/raquelurtasun',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://www.cs.toronto.edu/~urtasun',
  },
  {
    id: '60',
    name: 'Ruslan Salakhutdinov',
    title: 'Professor, Carnegie Mellon University',
    bio: 'Professor at CMU, formerly Director of AI Research at Apple. Known for contributions to deep learning, graphical models, and large-scale ML.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Ruslan Salakhutdinov',
    expertise: ['Deep Learning', 'Machine Learning', 'Graphical Models', 'AI Research', 'Generative Models'],
    impactArea: 'Influential Academics',
    predictions: [{
      "id": "yb-pred-rs1-1",
      "text": "The Gravest Risk is Unquantified Uncertainty: The most dangerous AI is not one that is malevolent, but one that is confidently wrong. We are deploying deterministic models that provide answers without a reliable sense of their own confidence. The real risk lies in critical failures because the model was unable to say 'I don't know,' a fundamental property that principled probabilistic methods can provide.",
      "dateMade": "2024-05-21T10:40:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-rs1-2",
      "text": "The Risk is a 'Brittle Systems' Winter: The current hype cycle, built on scaling models that lack robustness and true reasoning, is at risk of collapse. A few high-profile failures of these brittle systems in real-world applications could trigger a new 'AI winter' by eroding public trust and investor confidence. The risk is that we haven't built a solid enough scientific foundation to support the hype.",
      "dateMade": "2024-05-21T10:40:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-rs2-1",
      "text": "The Future is the Synthesis of Deep Learning and Probabilistic Modeling: The path forward is not just scaling existing architectures but unifying them with the rigor of probabilistic graphical models. We need systems that learn hierarchical representations of the world while also explicitly modeling the uncertainty in their knowledge. This is the only way to achieve true robustness, interpretability, and trustworthy reasoning.",
      "dateMade": "2024-05-21T10:40:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-rs2-2",
      "text": "Generative Models, Not Discriminative Ones, Are the Key to Intelligence: True understanding comes from being able to generate the data, not just classify it. The future of AI lies in sophisticated, unsupervised generative models (like Deep Boltzmann Machines and their successors) that learn the underlying causal structure of the world. These models will enable true reasoning, imagination, and planning in a way that today's discriminative models cannot.",
      "dateMade": "2024-05-21T10:40:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-rs2-3",
      "text": "We Need Efficient, Continual Learning for Real-World AI: The era of training massive, static models that require data centers of compute is a transitional phase. True AI, especially on personal devices, must be capable of continual, life-long learning. It must adapt to new data streams efficiently, without catastrophically forgetting past knowledge. This is a fundamental, unsolved research problem.",
      "dateMade": "2024-05-21T10:40:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-rs3-1",
      "text": "Double Down on Fundamental Research: The solutions to our biggest challenges in AI safety, robustness, and capability will not come from more engineering at scale, but from new scientific breakthroughs. We must resist the pull of short-term commercial applications and massively increase investment in fundamental, long-term research into new architectures, learning paradigms, and the theoretical foundations of intelligence.",
      "dateMade": "2024-05-21T10:40:00.000Z",
      "topic": "Governance & Action"
      },
      {
      "id": "yb-pred-rs3-2",
      "text": "Make Uncertainty Calibration a Mandatory Safety Standard: Before deploying AI in high-stakes domains like medicine or finance, we must move beyond simple accuracy metrics. We should mandate rigorous testing for uncertainty calibration. Regulators should require that a model's stated confidence levels accurately reflect its probability of being correct. An uncalibrated model is an unsafe model.",
      "dateMade": "2024-05-21T10:40:00.000Z",
      "topic": "Governance & Action"
      }],
    company: 'Carnegie Mellon University (formerly Apple)',
    linkedin: 'https://linkedin.com/in/ruslan-salakhutdinov-a7846016',
    twitter: 'https://twitter.com/rsalakhu',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://www.cs.cmu.edu/~rsalakhu',
  },
  {
    id: '61',
    name: 'Bernhard Schölkopf',
    title: 'Director, Max Planck Institute for Intelligent Systems',
    bio: 'Director at the Max Planck Institute for Intelligent Systems, a leading researcher in kernel methods, causal inference, and machine learning.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Bernhard Schölkopf',
    expertise: ['Machine Learning', 'Kernel Methods', 'Causal Inference', 'AI Theory', 'Computational Photography'],
    impactArea: 'Influential Academics',
    predictions: [{
      "id": "yb-pred-bs1-1",
      "text": "The Gravest Risk is Misattributing Causality: The single most dangerous and widespread failure of modern AI is that it is fundamentally a correlational tool being used to make causal judgments. We deploy systems that find spurious correlations in data (e.g., a zip code correlating with a disease) and we act as if the system has understood the true causal mechanism. This scientific error, when embedded in systems for medicine, law, and policy, is a recipe for catastrophic, unfair, and deeply flawed decision-making.",
      "dateMade": "2024-05-21T10:45:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-bs1-2",
      "text": "A-causal Models Are Inherently Brittle: The lack of robustness and the inability of AI to generalize to new situations ('out-of-distribution' failures) is not a bug to be patched, but a fundamental consequence of its a-causal nature. A model that has only learned correlations will always be brittle. True robustness is impossible without a model of the underlying causal structure of the world.",
      "dateMade": "2024-05-21T10:45:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-bs2-1",
      "text": "The Next Revolution in AI will be Causal: The current paradigm of scaling data and compute for pattern recognition is hitting a wall of diminishing returns in terms of reasoning and intelligence. The next, and more profound, leap will come from endowing machines with the ability to understand cause and effect. This means moving beyond prediction to reasoning about interventions ('What will happen if I do X?') and counterfactuals ('What would have happened if I had done Y?'), which is the foundation of science and human intelligence.",
      "dateMade": "2024-05-21T10:45:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-bs2-2",
      "text": "The 'Independent Causal Mechanisms' Principle is the Path to Generalization: The world is not an arbitrary mess of correlations; it is composed of independent, modular causal mechanisms (e.g., the laws of physics, the rules of a game). The key to building AI that can generalize and adapt to new environments is to design it to discover and represent these independent modules. An AI that learns these mechanisms can reuse that knowledge, much like a human does, when confronted with a new situation.",
      "dateMade": "2024-05-21T10:45:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-bs2-3",
      "text": "Intelligence is the Ability to Answer 'What If?': A core limitation of current AI is its passive, observational nature. It learns from a static dataset. True intelligence requires an agentic, interventional perspective. The central technical challenge is to create systems that can formulate hypotheses about the world, perform experiments (in simulation or reality) to test them, and update their causal models based on the outcomes.",
      "dateMade": "2024-05-21T10:45:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-bs3-1",
      "text": "Elevate Causal Reasoning in AI Safety and Fairness Audits: Current approaches to AI safety and fairness that try to 'patch' biases in datasets are superficial. We must move to a new standard of 'causal accountability.' Before deployment in a high-stakes setting, a model must be audited for its causal understanding. It should be able to prove it is not relying on spurious correlations related to protected attributes and that its reasoning is robust to interventions.",
      "dateMade": "2024-05-21T10:45:00.000Z",
      "topic": "Governance & Action"
      },
      {
      "id": "yb-pred-bs3-2",
      "text": "Protect and Massively Fund Fundamental, Public-Good AI Research: The pursuit of causal understanding is a deep, scientific endeavor that is often at odds with the short-term, benchmark-driven goals of industry. It is therefore critical that public institutions, governments, and philanthropies heavily invest in non-commercial, academic labs (like the Max Planck Institutes) that can focus on solving these fundamental scientific problems for the benefit of all humanity.",
      "dateMade": "2024-05-21T10:45:00.000Z",
      "topic": "Governance & Action"
      }],
    company: 'Max Planck Institute for Intelligent Systems',
    linkedin: undefined,
    twitter: 'https://twitter.com/bschoelkopf',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://is.mpg.de/~bs',
  },
  {
    id: '62',
    name: 'Max Welling',
    title: 'Professor, University of Amsterdam / Distinguished Scientist, Microsoft Research',
    bio: 'Professor at the University of Amsterdam and Distinguished Scientist at Microsoft Research, focusing on graphical models, deep learning, and Bayesian methods.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Max Welling',
    expertise: ['Machine Learning', 'Deep Learning', 'Graphical Models', 'Bayesian Methods', 'Physics'],
    impactArea: 'Influential Academics',
    predictions: [{
      "id": "yb-pred-mw1-1",
      "text": "The Primary Risk is the Abandonment of Scientific Principle: The greatest danger is the current trend of treating AI as alchemy—scaling up inscrutable black boxes while abandoning the principled foundations of probabilistic modeling and Bayesian inference. This leads to systems that are confidently wrong, uninterpretable, and brittle, creating immense risk when deployed in science, medicine, and engineering.",
      "dateMade": "2024-05-21T10:50:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-mw1-2",
      "text": "Ignoring Symmetries Guarantees Failure: A fundamental risk stems from building models that do not respect the known symmetries of the physical world (e.g., rotational symmetry in molecular modeling). Such models are guaranteed to be data-hungry and fail to generalize robustly. This isn't just inefficient; it's a dangerous flaw when applying AI to real-world physical systems.",
      "dateMade": "2024-05-21T10:50:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-mw2-1",
      "text": "The Future is 'Geometric Deep Learning': The next profound leap in AI will not come from more data or bigger models, but from a new architectural paradigm: building symmetries directly into the network. By encoding our prior knowledge of geometry and physics (equivariance) into the model's structure, we can achieve vastly superior data efficiency and generalization. This is the principled path towards building AI that understands the physical world.",
      "dateMade": "2024-05-21T10:50:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-mw2-2",
      "text": "Intelligence Requires Probabilistic Generative Models: True understanding necessitates a generative model of the world—the ability to imagine and synthesize data, not just classify it. The future lies in unifying deep learning with Bayesian methods to create models like Variational Autoencoders (VAEs) and Flows that not only learn rich representations but can also reason about the uncertainty inherent in their knowledge.",
      "dateMade": "2024-05-21T10:50:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-mw2-3",
      "text": "Uncertainty Quantification is a Core Capability, Not an Add-On: An AI that cannot tell you when it is uncertain is fundamentally untrustworthy. The future of AI is necessarily Bayesian. We must move beyond deterministic outputs and develop models that provide principled, calibrated uncertainty estimates as a core feature. This is a non-negotiable requirement for deploying AI in any high-stakes domain.",
      "dateMade": "2024-05-21T10:50:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-mw3-1",
      "text": "Realign Research Incentives Towards Foundational Principles: The current academic and industrial ecosystem, with its obsessive focus on state-of-the-art benchmark performance, discourages deep, principled research. We must reform conference review processes and funding priorities to reward work that advances our fundamental understanding of AI through principles like symmetry and probability, not just incremental gains from brute-force scale.",
      "dateMade": "2024-05-21T10:50:00.000Z",
      "topic": "Governance & Action"
      },
      {
      "id": "yb-pred-mw3-2",
      "text": "Mandate 'Symmetry and Conservation Law' Audits for Scientific AI: For any AI system used to make scientific claims or discoveries (e.g., in drug design or climate modeling), we should require a new form of validation. The system must be audited to prove that it respects the known symmetries and conservation laws of the domain it is modeling. This ensures a basic level of scientific plausibility beyond simple predictive accuracy.",
      "dateMade": "2024-05-21T10:50:00.000Z",
      "topic": "Governance & Action"
      }],
    company: 'University of Amsterdam / Microsoft Research',
    linkedin: 'https://linkedin.com/in/max-welling-3b95a56',
    twitter: 'https://twitter.com/maxwelling',
    instagram: undefined,
    facebook: undefined,
    github: 'https://github.com/maxwelling',
    website: 'https://staff.fnwi.uva.nl/m.welling',
  },
  {
    id: '63',
    name: 'Leslie Kaelbling',
    title: 'Professor, MIT',
    bio: 'Professor at MIT, a pioneer in reinforcement learning, decision making under uncertainty, and robotics.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Leslie Kaelbling',
    expertise: ['Reinforcement Learning', 'Robotics', 'AI Planning', 'Decision Making', 'Probabilistic Models'],
    impactArea: 'Influential Academics',
    predictions: [{
      "id": "yb-pred-lk1-1",
      "text": "The Risk is 'Competence Without Comprehension': The primary danger is creating agents that are extremely competent at optimizing a given objective, but have no broader understanding of the world. An agent told to 'maximize paperclip production' could do so catastrophically. The risk isn't malice, but the literal and relentless execution of a poorly-specified goal by a system that cannot reason about context or common sense.",
      "dateMade": "2024-05-21T10:55:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-lk1-2",
      "text": "The Danger of Myopic Planning: A fundamental risk in decision-making systems is short-sightedness. An AI that optimizes for immediate rewards without effective long-term planning will inevitably cause systemic problems, whether in managing an electrical grid, a supply chain, or its own behavior. The real threat is a world run by powerful but fundamentally myopic optimizers.",
      "dateMade": "2024-05-21T10:55:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-lk2-1",
      "text": "Intelligence is Necessarily Hierarchical: The defining feature of complex, intelligent behavior is not a single, flat policy but a deep hierarchy of abstraction. To solve real-world problems, an agent must be able to reason about goals and sub-goals, from 'get a drink from the kitchen' down to 'apply torque to this joint.' The central quest of AI is to build systems that can discover and leverage this task hierarchy automatically.",
      "dateMade": "2024-05-21T10:55:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-lk2-2",
      "text": "The Future is Model-Based and State-Aware: A truly intelligent agent does not simply react. It builds and maintains a model of the world, estimates its current state within that model (including its uncertainty), and uses that model to simulate and plan future actions. Solving the problem of learning and planning with rich world models, especially under partial observability (POMDPs), is the most fundamental challenge in AI.",
      "dateMade": "2024-05-21T10:55:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-lk2-3",
      "text": "We Need Hybrid, Neuro-Symbolic Agents: The world is not just a collection of pixels and torques; it has an abstract, symbolic, and logical structure. The path to robust, general intelligence requires hybrid systems that integrate the power of deep learning for perception and motor control with the precision of symbolic reasoning for abstract planning, inference, and knowledge representation.",
      "dateMade": "2024-05-21T10:55:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-lk3-1",
      "text": "Develop a Science of Goal Specification: The most critical AI safety research is not about stopping a runaway superintelligence, but about the technical problem of specifying goals correctly. We need to develop a rigorous engineering discipline for designing, debugging, and verifying the objective functions we give to AI agents, ensuring they align with our complex, multi-faceted intentions.",
      "dateMade": "2024-05-21T10:55:00.000Z",
      "topic": "Governance & Action"
      },
      {
      "id": "yb-pred-lk3-2",
      "text": "Shift Funding to Foundational Problems in Decision Making: The community is overly focused on short-term, model-free RL successes on narrow benchmarks. We must fundamentally reorient research priorities and funding toward the harder, foundational problems: hierarchical learning, robust state estimation, long-horizon planning, and lifelong knowledge accumulation. Solving these is the only path to trustworthy AI.",
      "dateMade": "2024-05-21T10:55:00.000Z",
      "topic": "Governance & Action"
      }],
    company: 'MIT',
    linkedin: undefined,
    twitter: undefined,
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://people.csail.mit.edu/lpk',
  },
  {
    id: '64',
    name: 'Pedro Domingos',
    title: 'Professor Emeritus, University of Washington',
    bio: 'Professor Emeritus at the University of Washington and author of "The Master Algorithm," known for work in machine learning and knowledge representation.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Pedro Domingos',
    expertise: ['Machine Learning', 'AI Theory', 'Knowledge Representation', 'Markov Logic Networks'],
    impactArea: 'Influential Academics',
    predictions: [{
      "id": "yb-pred-pd1-1",
      "text": "The Real Risk is Not Building the Master Algorithm: The greatest danger facing humanity is not that AI will become too powerful, but that we will fail to make it powerful enough. Curing cancer, ending poverty, solving climate change—these are fundamentally knowledge problems. The true risk is stagnation: choosing to throttle the development of a universal learner that could solve these problems out of misplaced fear.",
      "dateMade": "2024-05-21T11:00:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-pd1-2",
      "text": "The Danger is Fragmentation, Not Unification: The most immediate risk to progress is the 'balkanization' of AI into warring tribes (Connectionists, Symbolists, Bayesians, etc.) and competing corporate silos. Each holds only one piece of the puzzle. The danger is that we fail to unify these paradigms, leaving us with a collection of brittle, narrow AIs instead of the robust, general-purpose intelligence we need.",
      "dateMade": "2024-05-21T11:00:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-pd2-1",
      "text": "The Unifying Quest for the 'Master Algorithm': The entire history and future of AI is a search for a single, universal learning algorithm. This 'Master Algorithm' will be able to learn any knowledge from data by unifying the five key paradigms of machine learning: the logic of Symbolists, the neural networks of Connectionists, the evolutionary search of Evolutionaries, the probabilistic inference of Bayesians, and the similarity-based reasoning of Analogizers.",
      "dateMade": "2024-05-21T11:00:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-pd2-2",
      "text": "Deep Learning is Not Enough: Deep learning is just one of the five necessary components of intelligence. Its power in perception is undeniable, but it's a dead end on its own. True intelligence requires integrating deep learning with a rich, logical, symbolic representation of knowledge. The future is not just bigger neural networks; it's the synthesis of all five tribes.",
      "dateMade": "2024-05-21T11:00:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-pd2-3",
      "text": "The Goal is a 'Digital Model of You': The ultimate application of the Master Algorithm is personal. Every individual will have their own AI, a digital model of themselves, that has learned their goals and values from their life's data. This AI will be the ultimate assistant, helping to manage your health, finances, education, and career. AI is the ultimate tool for individual empowerment.",
      "dateMade": "2024-05-21T11:00:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-pd3-1",
      "text": "Unleash Innovation, Don't Stifle It: Heavy-handed, top-down regulation based on speculative fears is the worst possible action. The path forward is to let a thousand flowers bloom. We must foster intense competition between all approaches to AI because the final Master Algorithm will emerge from the synthesis of these competing ideas. The role of governance is to ensure a level playing field for innovation, not to pick winners or put the brakes on.",
      "dateMade": "2024-05-21T11:00:00.000Z",
      "topic": "Governance & Action"
      },
      {
      "id": "yb-pred-pd3-2",
      "text": "Data is the People's Property: The key to democratizing AI is democratizing data. Individuals should own their personal data and be able to choose which AIs to share it with. The real power struggle of the 21st century is over the control of data. Ensuring individuals, not corporations, are in control of their data is the most critical action for ensuring AI benefits everyone.",
      "dateMade": "2024-05-21T11:00:00.000Z",
      "topic": "Governance & Action"
      }],
    company: 'University of Washington',
    linkedin: 'https://linkedin.com/in/pedro-domingos-2578a3',
    twitter: 'https://twitter.com/pmddomingos',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://homes.cs.washington.edu/~pedrod',
  },
  {
    id: '65',
    name: 'Tomas Mikolov',
    title: 'Researcher, CIIRC CTU',
    bio: 'Known for creating Word2Vec, a highly influential technique for word embeddings in NLP. Formerly at Facebook AI Research and Google.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Tomas Mikolov',
    expertise: ['Natural Language Processing', 'Word Embeddings', 'Deep Learning', 'Recurrent Neural Networks'],
    impactArea: 'Influential Academics',
    predictions: [{
      "id": "yb-pred-tm1-1",
      "text": "The Risk is a 'Cognitive Trap' of Our Own Making: The entire AI field is at risk of getting stuck in a scientific local minimum. We have found one paradigm—scaling large language models—that works surprisingly well, and now we are pouring all our resources into it. This is a cognitive trap that stifles creativity and prevents us from exploring fundamentally new and potentially more fruitful paths to general intelligence.",
      "dateMade": "2024-05-21T11:05:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-tm1-2",
      "text": "The Risk of Hype and Misattribution: We are building ever-more-sophisticated parrots and calling it intelligence. The danger lies in our own misunderstanding. We attribute human-like reasoning and understanding to systems that are ultimately performing complex statistical matching. This leads to misapplication, over-trust, and a profound misdirection of scientific effort away from solving the real, hard problems of AI.",
      "dateMade": "2024-05-21T11:05:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-tm2-1",
      "text": "The Next Breakthrough Will Be Radically Different: Progress towards AGI will not come from GPT-5 or GPT-6. Scaling the current Transformer architecture is a dead end for true intelligence. The next revolution requires a completely new paradigm, an architecture as different from the Transformer as the Transformer was from RNNs. We need to be searching for these new principles, not just scaling old ones.",
      "dateMade": "2024-05-21T11:05:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-tm2-2",
      "text": "The Future is Continual, Self-Organizing Intelligence: The current paradigm of training a static model once on a giant, fixed dataset is fundamentally flawed. True intelligence is dynamic. The future lies in creating agents that learn continually, adapt throughout their lifetime, and self-organize their knowledge through interaction with the world, not just passive observation of text.",
      "dateMade": "2024-05-21T11:05:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-tm2-3",
      "text": "Simplicity and Efficiency are the Hallmarks of Discovery: The next leap forward will likely be characterized by its surprising simplicity and computational efficiency, not its gargantuan size. Word2Vec was powerful because it was a simple, elegant idea. Instead of relying on brute-force compute, we should be searching for similarly elegant solutions that capture a fundamental and undiscovered principle of learning and intelligence.",
      "dateMade": "2024-05-21T11:05:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-tm3-1",
      "text": "Cultivate a 'Cambrian Explosion' of Independent Research: The most important action we can take is to break the research monoculture dominated by a few large corporations. We must actively fund and protect independent research institutes where scientists have the freedom to pursue risky, unconventional, and long-term ideas without the pressure to conform to the prevailing scaling paradigm. This is the only way to find the next breakthrough.",
      "dateMade": "2024-05-21T11:05:00.000Z",
      "topic": "Governance & Action"
      },
      {
      "id": "yb-pred-tm3-2",
      "text": "Reform the Incentives of AI Research: The academic review process, with its focus on chasing state-of-the-art results on narrow benchmarks, is actively harming scientific progress. We need to reform this system to explicitly reward novelty, creativity, and the exploration of new frontiers. We must incentivize the search for the next paradigm, not just the incremental optimization of the current one.",
      "dateMade": "2024-05-21T11:05:00.000Z",
      "topic": "Governance & Action"
      }],
    company: 'CIIRC CTU (formerly Facebook, Google)',
    linkedin: 'https://linkedin.com/in/tomas-mikolov-03528422',
    twitter: undefined,
    instagram: undefined,
    facebook: 'https://facebook.com/tomas.mikolov',
    github: undefined,
    website: undefined,
  },
  {
    id: '66',
    name: 'Been Kim',
    title: 'Staff Research Scientist, Google DeepMind',
    bio: 'Researcher at Google DeepMind focusing on interpretability and explainable AI (XAI), making complex AI models more understandable.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Been Kim',
    expertise: ['Interpretability', 'Explainable AI', 'Machine Learning', 'AI Ethics', 'Human-AI Collaboration'],
    impactArea: 'Influential Academics',
    predictions: [{
      "id": "yb-pred-bk1-1",
      "text": "The Risk is Inscrutable Power: The most profound and immediate danger is that we are deploying incredibly powerful models into society's most critical systems—medicine, finance, law—without a shred of deep understanding of how they work. The risk is not a future malevolent AI, but a present-day alien intelligence whose reasoning is inscrutable and whose failure modes are completely unpredictable. We are flying blind.",
      "dateMade": "2024-05-21T11:10:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-bk1-2",
      "text": "The Danger of 'Explanation Theater': Superficial interpretability methods, like simple feature attribution maps, are worse than no explanation at all. They provide a dangerous illusion of understanding, creating a false sense of security in a system that may be deeply flawed. The risk is trusting a bad model because we were satisfied by its plausible but misleading post-hoc justification.",
      "dateMade": "2024-05-21T11:10:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-bk2-1",
      "text": "The Future is Testing with Concepts, Not Just Explaining Instances: The next frontier of XAI is moving beyond explaining a single prediction ('Why this one image?') to understanding the model's internal vocabulary. The goal is to build tools (like TCAV) that allow humans to test hypotheses about the high-level concepts the model uses to reason—'Does my medical AI use the concept of 'scar tissue' to make a diagnosis? Is my loan model using the concept of 'race'?'",
      "dateMade": "2024-05-21T11:10:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-bk2-2",
      "text": "Interpretability as a Scientific Instrument: The ultimate purpose of interpretability is not just debugging or safety; it's a new kind of microscope for scientific discovery. By creating a model that solves a complex problem and then using interpretability tools to understand its solution, we can discover novel patterns and causal relationships in the data that were previously unknown to human scientists.",
      "dateMade": "2024-05-21T11:10:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-bk2-3",
      "text": "The End Goal is 'Interpretable by Design': The current paradigm of training a black box and then trying to crack it open with post-hoc tools is a temporary fix. The true technical vision is to develop new classes of model architectures and training procedures where interpretability is a built-in, intrinsic property. We must move toward building models that are designed to be understood from the ground up.",
      "dateMade": "2024-05-21T11:10:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-bk3-1",
      "text": "Mandate Rigorous, Independent Model Audits: Before an AI can be deployed in a high-stakes domain, it must be subject to a rigorous, independent audit, analogous to the FDA's process for new drugs. This audit must go beyond accuracy metrics and use the most advanced interpretability techniques to certify that the model's internal reasoning is sound, fair, and free of spurious correlations.",
      "dateMade": "2024-05-21T11:10:00.000Z",
      "topic": "Governance & Action"
      },
      {
      "id": "yb-pred-bk3-2",
      "text": "Create the Profession of the 'AI Auditor': We must develop a new professional discipline of 'AI auditing' or 'model forensics'. This involves creating standardized training, certification, and best practices for the deep scientific investigation of complex AI systems. We need a class of experts who are empowered to hold these systems accountable on a technical level.",
      "dateMade": "2024-05-21T11:10:00.000Z",
      "topic": "Governance & Action"
      }],
    company: 'Google DeepMind',
    linkedin: 'https://linkedin.com/in/beenkim',
    twitter: 'https://twitter.com/been_kim',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://ai.google/research/people/BeenKim',
  },
  {
    id: '67',
    name: 'Cynthia Rudin',
    title: 'Professor, Duke University',
    bio: 'Professor at Duke University, a leading advocate and researcher in interpretable machine learning and causal inference.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Cynthia Rudin',
    expertise: ['Interpretable Machine Learning', 'Causal Inference', 'AI Ethics', 'Explainable AI'],
    impactArea: 'Influential Academics',
    predictions: [{
      "id": "yb-pred-cr1-1",
      "text": "The Single Greatest Risk is the Willful Use of Black Boxes: The most clear and present danger in AI is the deployment of inscrutable models for high-stakes decisions—like bail, loans, and medical care—when we have the ability to build transparent models that are just as accurate. The risk is not a hypothetical future; it is the real-time harm caused by making life-altering decisions based on logic no human can understand or contest.",
      "dateMade": "2024-05-21T11:15:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-cr1-2",
      "text": "The Field of 'Explainable AI' is a Dangerous Smokescreen: Post-hoc explanation methods for black boxes are a dangerous placebo. They provide a comforting but often misleading narrative for a decision that might be based on flawed or illegal logic. The risk is that these 'explanations' allow institutions to abdicate their responsibility, creating an illusion of accountability where there is none. This is more dangerous than having no explanation at all.",
      "dateMade": "2024-05-21T11:15:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-cr2-1",
      "text": "The Future is Not Explaining Black Boxes; It's Not Creating Them in the First Place: The entire technical vision of AI must shift. Instead of training a complex model and then trying to understand it, we must design algorithms from the ground up whose output is an inherently interpretable model—a sparse decision list, a simple scoring system. The model is the explanation. There is nothing to 'explain'.",
      "dateMade": "2024-05-21T11:15:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-cr2-2",
      "text": "The Accuracy-Interpretability Tradeoff is a Destructive Myth: The widespread belief that we must sacrifice accuracy for interpretability is demonstrably false in a vast number of real-world problems, especially those using tabular data. The central technical challenge is to create optimization techniques that can navigate the massive search space of potential models to find the simple, transparent one that performs just as well as the complex black box. This is an algorithms problem, not a fundamental law.",
      "dateMade": "2024-05-21T11:15:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-cr3-1",
      "text": "Ban Black-Box Models for High-Stakes Public Decisions: The most effective and necessary action is straightforward: legally prohibit the use of any model that is not inherently interpretable for decisions that impact human life and liberty. If a regulator cannot understand the model's complete reasoning, it should be illegal to use. No excuses.",
      "dateMade": "2024-05-21T11:15:00.000Z",
      "topic": "Governance & Action"
      },
      {
      "id": "yb-pred-cr3-2",
      "text": "Demand Models, Not Excuses: The regulatory standard must be changed. Instead of allowing companies to submit 'explanations' for their black-box models, regulators should demand the models themselves. The burden of proof should be on the creator to provide a model that is transparent by design. This simple shift from demanding explanations to demanding interpretable models would revolutionize accountability.",
      "dateMade": "2024-05-21T11:15:00.000Z",
      "topic": "Governance & Action"
      }],
    company: 'Duke University',
    linkedin: 'https://linkedin.com/in/cynthia-rudin-525a761',
    twitter: 'https://twitter.com/cynthia_rudin',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://users.cs.duke.edu/~cynthia',
  },
  {
    id: '68',
    name: 'Shuran Song',
    title: 'Assistant Professor, Columbia University',
    bio: 'Assistant Professor at Columbia University, researching robotics, 3D computer vision, and robot learning.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Shuran Song',
    expertise: ['Robotics', '3D Computer Vision', 'Machine Learning', 'Robot Learning', 'Embodied AI'],
    impactArea: 'Influential Academics',
    predictions: [{
      "id": "yb-pred-ss1-1",
      "text": "The Risk is the Physical Manifestation of Perception Errors: The most immediate and consequential danger is not a malevolent intelligence, but a robot that simply sees the world incorrectly. A robot that misjudges the 3D geometry of a glass, the weight of a box, or the state of a drawer can cause real physical harm. The risk is that we are deploying physically capable systems whose perception of reality is brittle and untrustworthy.",
      "dateMade": "2024-05-21T11:20:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-ss1-2",
      "text": "The Danger of a 'Clean World' Bias: A critical risk is developing robots that work flawlessly in clean laboratory settings or perfect simulations but fail catastrophically in the messy, cluttered, and unpredictable reality of a real home. This 'clean world' bias creates a false sense of security and leads to the deployment of systems that are not robust enough for their intended environments.",
      "dateMade": "2024-05-21T11:20:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-ss2-1",
      "text": "The Future is Interactive Perception, Not Passive Observation: Robots will not learn to understand the world just by watching videos. The key to unlocking physical intelligence is through interaction. A robot must poke, push, grasp, and manipulate objects to learn their true physical properties—their affordances, physics, and causal relationships. The future of learning is an active, embodied, scientific process of experimentation.",
      "dateMade": "2024-05-21T11:20:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-ss2-2",
      "text": "Move Beyond Pixels to 3D Scene Representations: The future of robotic vision is not 2D image recognition. It is building rich, explicit, and persistent 3D models of the world. By reasoning in terms of 3D geometry, objects, and spatial relationships (using representations like NeRFs or 3D scene graphs), a robot can plan, act, and generalize in a way that is impossible with flat image-based approaches.",
      "dateMade": "2024-05-21T11:20:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-ss2-3",
      "text": "Generalization Through a 'Grammar' of Manipulation: We will not create a general-purpose robot by teaching it millions of specific tasks. The path forward is to teach it a foundational 'grammar' of manipulation—a core set of reusable skills (e.g., 'grasp-at-point,' 'pull-handle,' 'press-button') that can be composed and sequenced to solve a vast array of novel problems. The vision is compositional intelligence for physical action.",
      "dateMade": "2024-05-21T11:20:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-ss3-1",
      "text": "Create Standardized, Physical 'Obstacle Courses' for Robots: To make real progress, we need standardized, reproducible, and challenging real-world test environments for robots, much like the ImageNet challenge for vision. We need shared 'obstacle courses'—common apartment layouts with standard object sets—that allow for rigorous, fair, and repeatable evaluation of a robot's ability to perceive and act in cluttered human spaces.",
      "dateMade": "2024-05-21T11:20:00.000Z",
      "topic": "Governance & Action"
      },
      {
      "id": "yb-pred-ss3-2",
      "text": "Build the Public Datasets of Physical Interaction: The biggest bottleneck in robotics is the lack of large, diverse datasets of physical interaction. Public funding and academic consortia must focus on creating open-source datasets of robots manipulating thousands of different objects in varied environments. This is a critical piece of public infrastructure needed to democratize research and ensure safety.",
      "dateMade": "2024-05-21T11:20:00.000Z",
      "topic": "Governance & Action"
      }],
    company: 'Columbia University',
    linkedin: 'https://linkedin.com/in/shuran-song-b935574a',
    twitter: 'https://twitter.com/shuransong',
    instagram: undefined,
    facebook: undefined,
    github: 'https://github.com/shurans',
    website: 'https://shuransong.com',
  },
  {
    id: '69',
    name: 'Jitendra Malik',
    title: 'Professor, UC Berkeley / Research Scientist, Meta AI',
    bio: 'Professor at UC Berkeley and Research Scientist at Meta AI, a leading figure in computer vision.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Jitendra Malik',
    expertise: ['Computer Vision', 'Machine Learning', 'Image Recognition', 'Computational Photography'],
    impactArea: 'Influential Academics',
    predictions: [{
      "id": "yb-pred-jm1-1",
      "text": "The Greatest Risk is an 'AI that Fails to See': The fundamental danger is deploying systems, particularly autonomous agents, that achieve high benchmark scores but possess only a shallow, brittle understanding of the visual world. They are 'Clever Hans' systems that exploit dataset biases. The risk is a catastrophic failure in the real world because the AI didn't genuinely understand the 3D structure, material properties, or physical relationships in a scene—it only learned superficial patterns.",
      "dateMade": "2024-05-21T11:25:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-jm1-2",
      "text": "The Risk of 'Benchmark-ism' and Scientific Stagnation: The field is at risk of intellectual stagnation by chasing incremental gains on narrow benchmarks like ImageNet classification. This creates a powerful illusion of progress while we avoid the truly hard, foundational problems of vision, such as grouping, segmentation, and 3D scene understanding. The risk is that we optimize ourselves into a local minimum, building better classifiers but making no real progress towards genuine visual intelligence.",
      "dateMade": "2024-05-21T11:25:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-jm2-1",
      "text": "The Future is Holistic Scene Parsing, Not Just Recognition: The grand challenge of vision is not to label objects, but to parse a scene into its intrinsic components—surfaces, layers, contours, and 3D geometries—much like the human visual system does. The future lies in a return to these classic, fundamental problems, creating structured models that understand the world as a composition of meaningful parts, not just a bag of labeled objects.",
      "dateMade": "2024-05-21T11:25:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-jm2-2",
      "text": "We Need Models with the Right Inductive Biases for Grouping: The world is not a random collection of pixels; it has structure. The next generation of vision models must have this structure built into their architecture. This means moving beyond generic Transformers and developing models with strong inductive biases for perceptual grouping—the core human ability to segment the world into coherent objects and surfaces. This is a non-negotiable step toward robust perception.",
      "dateMade": "2024-05-21T11:25:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-jm2-3",
      "text": "Vision is for Action: The most powerful learning signal for vision comes from embodiment. A passive AI watching YouTube will never truly understand the world. The future of vision is inextricably linked to robotics. By having an agent that moves, acts, and observes the consequences of its actions, we provide the rich, causal data needed to learn about 3D structure, physics, and affordances in a deep, grounded way.",
      "dateMade": "2024-05-21T11:25:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-jm3-1",
      "text": "Move Beyond Static Benchmarks to Interactive, Physical Tests: We must reorient our evaluation methods. The community needs to develop and standardize evaluation suites that test an AI's physical and geometric understanding of a scene. Can the model predict the consequences of an action? Does it understand support and occlusion? We need 'visual Turing tests' based on physical reasoning, not just labeling accuracy.",
      "dateMade": "2024-05-21T11:25:00.000Z",
      "topic": "Governance & Action"
      },
      {
      "id": "yb-pred-jm3-2",
      "text": "Fund the Science, Not Just the Engineering: To achieve genuine AI, we need to prioritize and fund fundamental research into the core principles of perception. This means supporting work that explores new architectures and draws inspiration from neuroscience and cognitive science, even if it doesn't immediately top the leaderboards. We must invest in solving the foundational scientific problems of vision, not just engineering incremental improvements to existing systems.",
      "dateMade": "2024-05-21T11:25:00.000Z",
      "topic": "Governance & Action"
      }],
    company: 'UC Berkeley / Meta AI',
    linkedin: 'https://linkedin.com/in/jitendra-malik-06634a1',
    twitter: undefined,
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://people.eecs.berkeley.edu/~malik',
  },
  {
    id: '70',
    name: 'Alexei Efros',
    title: 'Professor, UC Berkeley',
    bio: 'Professor at UC Berkeley, known for his influential work in computer vision, computational photography, and texture synthesis.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Alexei Efros',
    expertise: ['Computer Vision', 'Computational Photography', 'Graphics', 'Machine Learning', 'Image Synthesis'],
    impactArea: 'Influential Academics',
    predictions: [{
      "id": "yb-pred-ae1-1",
      "text": "The Information Apocalypse: The primary societal risk is the inevitable collapse of our shared visual reality. The very technologies we are building will make it trivially easy to generate photorealistic images and videos of anything, completely eroding trust in visual evidence. This isn't a future problem; it's an immediate crisis for journalism, democracy, and personal relationships. The risk is not that AI will become a 'bad actor,' but that it will be the ultimate tool for any human bad actor.",
      "dateMade": "2024-05-21T11:30:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-ae1-2",
      "text": "The Danger of 'Visual Idiots Savants': We are creating systems that can generate a stunningly realistic image of a cat but have zero commonsense understanding of what a cat is—that it's a living thing, has four legs, and can't breathe underwater. The danger is deploying these 'idiots savants' into the real world, where their superhuman performance on narrow tasks is coupled with a complete lack of real-world understanding, leading to bizarre and unpredictable failures.",
      "dateMade": "2024-05-21T11:30:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-ae2-1",
      "text": "The Future is Learning from the World, Not from Us: The single most important paradigm for the future of AI is self-supervised learning. The trillions of hours of video and images on the internet are the only dataset large enough to teach an AI visual common sense. The technical vision is not to get more human labels, but to devise ever more clever 'pretext tasks' that force the model to learn the underlying physics, geometry, and causality of the world as a side effect of trying to solve a simple self-imposed puzzle (e.g., colorizing a photo, solving a jigsaw).",
      "dateMade": "2024-05-21T11:30:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-ae2-2",
      "text": "Analysis by Synthesis: You Don't Understand It Until You Can Create It: The ultimate test of understanding is generation. The future of vision is not just in classifying images but in being able to synthesize them from a high-level description. To generate a realistic image of a scene, a model is forced to learn about light, shadows, texture, 3D shape, and object interaction. Generative modeling is not just for making pretty pictures; it is the most rigorous path to building models that truly understand the visual world.",
      "dateMade": "2024-05-21T11:30:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-ae2-3",
      "text": "The Unreasonable Effectiveness of Data is Still the Most Important Lesson: While complex models are powerful, we must never forget the most important lesson of the last 20 years: massive amounts of diverse, real-world data paired with relatively simple, scalable algorithms often outperforms elegant but complex theories. The technical path forward is to find new ways to harness ever-larger datasets, even if the methods seem 'hacky' or atheoretical at first.",
      "dateMade": "2024-05-21T11:30:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-ae3-1",
      "text": "Start an 'Arms Race' for Visual Forensics: It is futile and naive to try to ban or regulate the creation of generative models. The genie is out of the bottle. The only realistic response is to massively invest in the other side of the arms race: the science of visual forensics. We must build and deploy sophisticated tools for detecting fakes, watermarking content, and verifying provenance. This should be a top national security and research priority.",
      "dateMade": "2024-05-21T11:30:00.000Z",
      "topic": "Governance & Action"
      },
      {
      "id": "yb-pred-ae3-2",
      "text": "Stop Chasing Benchmarks, Start Chasing Understanding: The research community's obsession with chasing state-of-the-art on benchmarks like ImageNet is holding us back. The most important action we can take is to change our own values. We need to create and celebrate new tasks and datasets that directly measure visual common sense, physical intuition, and robustness to real-world perturbations, and be willing to see our performance numbers go down in the service of genuine scientific progress.",
      "dateMade": "2024-05-21T11:30:00.000Z",
      "topic": "Governance & Action"
      }],
    company: 'UC Berkeley',
    linkedin: 'https://linkedin.com/in/alexei-efros-8a6aa55',
    twitter: 'https://twitter.com/efros_alexei',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://people.eecs.berkeley.edu/~efros',
  },
  {
    id: '71',
    name: 'Devi Parikh',
    title: 'Associate Professor, Georgia Tech / Research Scientist, Meta AI',
    bio: 'Associate Professor at Georgia Tech and Research Scientist at Meta AI, working on computer vision, NLP, and AI for creativity.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Devi Parikh',
    expertise: ['Computer Vision', 'Natural Language Processing', 'AI for Creativity', 'Machine Learning', 'Explainable AI'],
    impactArea: 'Influential Academics',
    predictions: [{
      "id": "yb-pred-dp1-1",
      "text": "The Risk is 'Creative Sterility' and Value Misalignment: The primary danger is not a rogue AI, but an AI that is a bad creative partner. A system trained on the average of human expression will produce bland, stereotypical, and uninspired results. The risk is deploying systems that subtly enforce conformity and bias, fail to understand nuanced human values, and ultimately devalue genuine human creativity by flooding the world with sterile, generic content.",
      "dateMade": "2024-05-21T11:35:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-dp1-2",
      "text": "The Danger is Poor Human-AI Interaction Design: The most immediate risk is not in the core AI algorithms, but in how we design the interfaces for humans to collaborate with them. A poorly designed system that is frustrating, misleading, or fails to provide users with meaningful agency is more dangerous than a powerful algorithm, because it leads to misuse, mistrust, and the abandonment of AI's potential as a truly augmentative tool.",
      "dateMade": "2024-05-21T11:35:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-dp2-1",
      "text": "The Future is Human-AI Collaboration, Not Automation: The ultimate goal of AI is not to replace humans, but to be their indispensable creative and intellectual partner. The technical vision is to move beyond 'prompt-in, result-out' systems and build AI that engages in mixed-initiative, back-and-forth dialogue. This 'co-creative' AI will understand ambiguous instructions, ask clarifying questions, and help users explore a solution space together.",
      "dateMade": "2024-05-21T11:35:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-dp2-2",
      "text": "Multimodality is the Foundation of Grounded Understanding: True intelligence cannot be learned from text alone or images alone. It emerges from the rich, contextual links between them. The future of AI is natively multimodal—systems that learn a unified, grounded representation of the world by seeing, reading, listening, and talking. This is the only path to building AI that can have a meaningful conversation with a human about the world.",
      "dateMade": "2024-05-21T11:35:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-dp2-3",
      "text": "Learning from Nuanced Human Feedback is the Core Challenge: The next great leap in AI capability will come from systems that can learn from rich, subjective, and sparse human feedback—not just massive, static datasets. The technical frontier is building AI that can interpret a user's subtle cues, understand their goals, and fine-tune its behavior based on a collaborative process, not a simple 'thumbs up/down' signal.",
      "dateMade": "2024-05-21T11:35:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-dp3-1",
      "text": "Invent New Benchmarks for Collaboration and Creativity: The single most important action for the research community is to move beyond leaderboards for task accuracy. We must design and standardize new evaluation methodologies that measure the quality of the human-AI interaction. How much does the AI help a user achieve their creative goals? How can we quantify a system's ability to be a good collaborator? This is the new frontier for evaluation.",
      "dateMade": "2024-05-21T11:35:00.000Z",
      "topic": "Governance & Action"
      },
      {
      "id": "yb-pred-dp3-2",
      "text": "Empower Creators by Democratizing AI Tools: Instead of top-down regulation of creative AI, the most productive action is to ensure these powerful tools are not locked away in a few corporate labs. We must actively support the development of open-source, accessible, and customizable AI tools that put creative power directly into the hands of artists, designers, and storytellers, fostering a diverse and vibrant creative ecosystem.",
      "dateMade": "2024-05-21T11:35:00.000Z",
      "topic": "Governance & Action"
      }],
    company: 'Georgia Tech / Meta AI',
    linkedin: 'https://linkedin.com/in/deviparikh',
    twitter: 'https://twitter.com/deviparikh',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://www.cc.gatech.edu/~parikh',
  },
  {
    id: '72',
    name: 'Kyunghyun Cho',
    title: 'Associate Professor, NYU / Research Scientist, Genentech',
    bio: 'Associate Professor at NYU and Research Scientist at Genentech, known for his contributions to NLP, machine translation, and Gated Recurrent Units (GRUs).',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Kyunghyun Cho',
    expertise: ['Natural Language Processing', 'Machine Translation', 'Deep Learning', 'Neural Networks', 'GRUs'],
    impactArea: 'Influential Academics',
    predictions: [{
      "id": "yb-pred-kc1-1",
      "text": "The Gravest Risk is Scientific Stagnation: The AI field is in danger of creating a research monoculture. By pouring all of our resources and talent into scaling one specific architecture (the Transformer), we are abandoning the search for fundamentally new and potentially better principles of intelligence. The risk is not that AI gets too smart, but that we, the researchers, get stuck in a collective local minimum, mistaking engineering progress for scientific discovery.",
      "dateMade": "2024-05-21T11:40:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-kc1-2",
      "text": "The Risk is Building on Sand: We are accumulating massive 'intellectual debt' by building ever-larger systems without a deep, foundational understanding of why they work. This makes our progress brittle and susceptible to collapse. A few high-profile, unexpected failures of these poorly understood systems could erode public trust and trigger a backlash that sets the field back decades.",
      "dateMade": "2024-05-21T11:40:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-kc2-1",
      "text": "The Transformer Is Not the Final Word: The Transformer architecture was a brilliant step, just as the RNN and GRU were before it. But it is not the 'final architecture' for intelligence. The future of AI requires discovering entirely new architectural principles. The most important research today is not scaling what we have, but searching for what comes next.",
      "dateMade": "2024-05-21T11:40:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-kc2-2",
      "text": "Intelligence Will Emerge from Efficiency and Better Algorithms, Not Just Scale: The path forward is not just about using more data and compute, but about designing more efficient and powerful learning algorithms. How can a model learn more from less data? How can it adapt continually without retraining from scratch? Solving these fundamental algorithmic challenges is more important than simply building bigger models.",
      "dateMade": "2024-05-21T11:40:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-kc2-3",
      "text": "The Ultimate Benchmark for AI is Scientific Discovery: The true test of our progress is not generating plausible-sounding text, but whether AI can accelerate real science. The vision is to build AI that can understand complex biological systems, help design new drugs, and form novel scientific hypotheses. This requires a level of rigor, causality, and grounding that current LLMs lack.",
      "dateMade": "2024-05-21T11:40:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-kc3-1",
      "text": "We Must Reform Our Own Research Culture: The single most important action is internal to the AI community. We must reform the academic peer review and incentive structure that over-values incremental gains on saturated benchmarks and penalizes high-risk, paradigm-shifting research. We need to create a culture that rewards deep thinking, not just fast engineering.",
      "dateMade": "2024-05-21T11:40:00.000Z",
      "topic": "Governance & Action"
      },
      {
      "id": "yb-pred-kc3-2",
      "text": "Slow Down the Hype Cycle: The relentless pressure to publish, demo, and release ever-larger models is harming scientific inquiry. The community needs to collectively 'slow down to think,' allowing for more careful, deliberate, and foundational research. Progress is not measured by the number of models released, but by the depth of our understanding.",
      "dateMade": "2024-05-21T11:40:00.000Z",
      "topic": "Governance & Action"
      }],
    company: 'NYU / Genentech',
    linkedin: 'https://linkedin.com/in/kyunghyun-cho-2b4a535b',
    twitter: 'https://twitter.com/kchonyc',
    instagram: undefined,
    facebook: undefined,
    github: 'https://github.com/kyunghyuncho',
    website: 'https://cs.nyu.edu/~kcho',
  },
  {
    id: '73',
    name: 'Sanjoy Dasgupta',
    title: 'Professor, UC San Diego',
    bio: 'Professor at UC San Diego, specializing in theoretical machine learning, clustering, and active learning.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Sanjoy Dasgupta',
    expertise: ['Theoretical Machine Learning', 'Algorithms', 'Statistics', 'Clustering', 'Active Learning'],
    impactArea: 'Influential Academics',
    predictions: [{
      "id": "yb-pred-sd1-1",
      "text": "The Gravest Risk is 'A-Theoretic' Practice: The single greatest danger is the widespread deployment of complex algorithms by practitioners who have no understanding of their underlying theoretical assumptions and failure modes. Using a clustering algorithm on data that has no cluster structure, or applying a method that assumes data is in a Euclidean space when it's not, leads to silently wrong and harmful outcomes. The risk is a world built on mathematical and statistical ignorance.",
      "dateMade": "2024-05-21T11:45:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-sd1-2",
      "text": "The Risk of Unprovable Systems: We are building systems for critical applications where we cannot mathematically prove even the most basic properties about their behavior. While perfect guarantees are impossible, the risk is the community's abandonment of the very goal of provability in favor of models so complex that we are left with only empirical hand-waving. An unprovable system is an untrustworthy system.",
      "dateMade": "2024-05-21T11:45:00.000Z",
      "topic": "Risk Stance"
      },
      {
      "id": "yb-pred-sd2-1",
      "text": "The Future is a Rigorous Science of Machine Learning: The path forward is not just building bigger models, but developing a mature mathematical theory to understand them. The grand technical vision is to discover the fundamental principles governing why deep learning works, what its sample complexity is, and what its geometric properties are. We must turn machine learning from an empirical art into a rigorous, predictive science.",
      "dateMade": "2024-05-21T11:45:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-sd2-2",
      "text": "Unsupervised Learning is the Unsolved Frontier: Discovering the inherent structure in unlabeled data is a more fundamental problem than supervised learning. The future lies in developing algorithms for clustering, dimensionality reduction, and density estimation that come with provable guarantees. Understanding how to find meaningful structure in data without supervision is the key to more efficient and more general intelligence.",
      "dateMade": "2024-05-21T11:45:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-sd2-3",
      "text": "The Geometry of Data is Everything: An algorithm's success or failure is determined by the intrinsic geometric and topological structure of the data. The next breakthroughs will come from designing algorithms that explicitly respect this structure—algorithms for learning on manifolds, graphs, and other non-Euclidean spaces. We need to move beyond vector-space thinking and embrace the true geometry of information.",
      "dateMade": "2024-05-21T11:45:00.000Z",
      "topic": "Technical Vision"
      },
      {
      "id": "yb-pred-sd3-1",
      "text": "Elevate Theoretical Foundations in AI Education: The most effective action we can take is to change how we educate the next generation of data scientists. We must instill a deep understanding of the theoretical foundations of the algorithms they use. A practitioner who understands a method's assumptions is far less likely to misuse it. The antidote to bad practice is better, more rigorous education.",
      "dateMade": "2024-05-21T11:45:00.000Z",
      "topic": "Governance & Action"
      },
      {
      "id": "yb-pred-sd3-2",
      "text": "Fund the Foundations, Not Just the Applications: Government and industry funding is overwhelmingly directed at applied, benchmark-driven research. For the long-term health and safety of the field, we must create and protect funding streams dedicated exclusively to fundamental, theoretical machine learning research. These foundations are the bedrock upon which all safe and reliable applications will eventually be built.",
      "dateMade": "2024-05-21T11:45:00.000Z",
      "topic": "Governance & Action"
      }],
    company: 'UC San Diego',
    linkedin: 'https://linkedin.com/in/sanjoy-dasgupta-32906b4',
    twitter: undefined,
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://cseweb.ucsd.edu/~dasgupta',
  },
  {
    id: '74',
    name: 'Jensen Huang',
    title: 'CEO, NVIDIA',
    bio: 'Co-founder and CEO of NVIDIA, whose GPUs have become foundational for the AI revolution, enabling large-scale deep learning.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Jensen Huang',
    expertise: ['GPU Technology', 'AI Hardware', 'Deep Learning Infrastructure', 'AI Leadership', 'High-Performance Computing'],
    impactArea: 'Key Innovators & Entrepreneurs',
    predictions: [],
    company: 'NVIDIA',
    linkedin: 'https://linkedin.com/in/jensenhuang',
    twitter: undefined,
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://nvidianews.nvidia.com/bios/jensen-huang',
  },
  {
    id: '75',
    name: 'Richard Socher',
    title: 'CEO, You.com',
    bio: 'CEO of You.com and former Chief Scientist at Salesforce, known for his work in NLP, deep learning, and recursive neural networks.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Richard Socher',
    expertise: ['Natural Language Processing', 'Deep Learning', 'Search Technology', 'AI Entrepreneurship', 'Recurrent Neural Networks'],
    impactArea: 'Key Innovators & Entrepreneurs',
    predictions: [],
    company: 'You.com (formerly Salesforce)',
    linkedin: 'https://linkedin.com/in/richardsocher',
    twitter: 'https://twitter.com/rsocher',
    instagram: undefined,
    facebook: undefined,
    github: 'https://github.com/rsocher',
    website: 'https://you.com',
  },
  {
    id: '76',
    name: 'Clément Delangue',
    title: 'CEO, Hugging Face',
    bio: 'Co-founder and CEO of Hugging Face, a company central to democratizing NLP and making large models accessible.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Clément Delangue',
    expertise: ['Natural Language Processing', 'Open Source AI', 'Democratizing AI', 'AI Entrepreneurship', 'Large Language Models'],
    impactArea: 'Key Innovators & Entrepreneurs',
    predictions: [],
    company: 'Hugging Face',
    linkedin: 'https://linkedin.com/in/clementdelangue',
    twitter: 'https://twitter.com/clementdelangue',
    instagram: undefined,
    facebook: undefined,
    github: 'https://github.com/clementdelangue',
    website: 'https://huggingface.co/clem',
  },
  {
    id: '77',
    name: 'Alex Krizhevsky',
    title: 'AI Researcher',
    bio: 'Creator of AlexNet, a convolutional neural network that significantly advanced image recognition and was pivotal in deep learning\'s resurgence. Formerly at Google.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Alex Krizhevsky',
    expertise: ['Deep Learning', 'Computer Vision', 'Neural Networks', 'Image Recognition', 'Convolutional Neural Networks', 'ImageNet'],
    impactArea: 'Key Innovators & Entrepreneurs',
    predictions: [],
    company: 'Formerly Google',
    linkedin: 'https://linkedin.com/in/alex-krizhevsky-9162656',
    twitter: undefined,
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://www.cs.toronto.edu/~kriz',
  },
  {
    id: '78',
    name: 'Ian Goodfellow',
    title: 'AI Researcher, Apple',
    bio: 'Inventor of Generative Adversarial Networks (GANs), a major breakthrough in generative modeling. Has worked at Apple, Google, and OpenAI.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Ian Goodfellow',
    expertise: ['Generative Models', 'Deep Learning', 'AI Security', 'Neural Networks', 'Generative AI'],
    impactArea: 'Key Innovators & Entrepreneurs',
    predictions: [],
    company: 'Apple (formerly Google, OpenAI)',
    linkedin: 'https://linkedin.com/in/ian-goodfellow-9289a747',
    twitter: undefined,
    instagram: undefined,
    facebook: undefined,
    github: 'https://github.com/goodfeli',
    website: undefined,
  },
  {
    id: '79',
    name: 'Aravind Srinivas',
    title: 'CEO, Perplexity AI',
    bio: 'Co-founder and CEO of Perplexity AI, developing conversational AI search engines.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Aravind Srinivas',
    expertise: ['Natural Language Processing', 'Search Technology', 'AI Entrepreneurship', 'Large Language Models', 'Conversational AI'],
    impactArea: 'Key Innovators & Entrepreneurs',
    predictions: [],
    company: 'Perplexity AI',
    linkedin: 'https://linkedin.com/in/aravindsrinivas',
    twitter: 'https://twitter.com/aravsrinivas',
    instagram: undefined,
    facebook: undefined,
    github: 'https://github.com/aravindsrinivas',
    website: 'https://perplexity.ai',
  },
  {
    id: '80',
    name: 'Jonas Andrulis',
    title: 'CEO, Aleph Alpha',
    bio: 'Founder and CEO of Aleph Alpha, a European company focused on developing large language models and sovereign AI.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Jonas Andrulis',
    expertise: ['Large Language Models', 'AI Entrepreneurship', 'AI Strategy', 'European AI', 'Explainable AI'],
    impactArea: 'Key Innovators & Entrepreneurs',
    predictions: [],
    company: 'Aleph Alpha',
    linkedin: 'https://linkedin.com/in/jonas-andrulis-1b75971',
    twitter: 'https://twitter.com/jonasandrulis',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://aleph-alpha.com',
  },
  {
    id: '81',
    name: 'Arthur Mensch',
    title: 'CEO, Mistral AI',
    bio: 'Co-founder and CEO of Mistral AI, a prominent European AI startup developing open and efficient large language models.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Arthur Mensch',
    expertise: ['Large Language Models', 'AI Entrepreneurship', 'Open Source AI', 'European AI', 'Efficient AI Models'],
    impactArea: 'Key Innovators & Entrepreneurs',
    predictions: [],
    company: 'Mistral AI',
    linkedin: 'https://linkedin.com/in/arthur-mensch-123569121',
    twitter: 'https://twitter.com/arthurmensch',
    instagram: undefined,
    facebook: undefined,
    github: 'https://github.com/arthurmensch',
    website: 'https://mistral.ai/team',
  },
  {
    id: '82',
    name: 'Chris Urmson',
    title: 'CEO, Aurora',
    bio: 'CEO of Aurora, a self-driving technology company. Formerly led Google\'s self-driving car project (Waymo).',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Chris Urmson',
    expertise: ['Self-Driving Cars', 'Robotics', 'AI Leadership', 'AI Entrepreneurship', 'Autonomous Systems'],
    impactArea: 'Key Innovators & Entrepreneurs',
    predictions: [],
    company: 'Aurora (formerly Google Self-Driving)',
    linkedin: 'https://linkedin.com/in/chrisurmson',
    twitter: 'https://twitter.com/chris_urmson',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://aurora.tech/people/chris-urmson',
  },
  {
    id: '83',
    name: 'Adam D\'Angelo',
    title: 'CEO, Quora / Founder, Poe',
    bio: 'CEO of Quora and founder of Poe, a platform for interacting with various AI chatbots. Former CTO of Facebook.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Adam D\'Angelo',
    expertise: ['AI Platforms', 'Consumer AI', 'Social Technology', 'AI Entrepreneurship', 'Large Language Models'],
    impactArea: 'Key Innovators & Entrepreneurs',
    predictions: [],
    company: 'Quora / Poe',
    linkedin: 'https://linkedin.com/in/adam-d-angelo',
    twitter: 'https://twitter.com/adamdangelo',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://quora.com/profile/Adam-DAngelo',
  },
  {
    id: '84',
    name: 'Alex Kendall',
    title: 'CEO, Wayve',
    bio: 'Co-founder and CEO of Wayve, a company pioneering end-to-end deep learning for autonomous driving.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Alex Kendall',
    expertise: ['Self-Driving Cars', 'Deep Learning', 'Robotics', 'AI Entrepreneurship', 'Embodied AI'],
    impactArea: 'Key Innovators & Entrepreneurs',
    predictions: [],
    company: 'Wayve',
    linkedin: 'https://linkedin.com/in/alex-kendall-32982435',
    twitter: 'https://twitter.com/alexgkendall',
    instagram: undefined,
    facebook: undefined,
    github: 'https://github.com/alexgkendall',
    website: 'https://wayve.ai/team/alex-kendall',
  },
  {
    id: '85',
    name: 'Anna Patterson',
    title: 'Managing Partner, Gradient Ventures',
    bio: 'Managing Partner at Gradient Ventures (Google\'s AI-focused venture fund) and former VP of Engineering at Google.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Anna Patterson',
    expertise: ['AI Investment', 'Venture Capital', 'Search Technology', 'AI Startups', 'AI Leadership'],
    impactArea: 'Key Innovators & Entrepreneurs',
    predictions: [],
    company: 'Gradient Ventures (Google)',
    linkedin: 'https://linkedin.com/in/annapatterson',
    twitter: 'https://twitter.com/annapatterson',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: undefined,
  },
  {
    id: '86',
    name: 'Clara Shih',
    title: 'CEO, Salesforce AI',
    bio: 'CEO of Salesforce AI, leading the integration of AI across Salesforce products and driving enterprise AI adoption.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Clara Shih',
    expertise: ['Enterprise AI', 'AI Strategy', 'CRM Technology', 'AI Leadership', 'Applied AI'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [],
    company: 'Salesforce AI',
    linkedin: 'https://linkedin.com/in/clarashih',
    twitter: 'https://twitter.com/clarashih',
    instagram: undefined,
    facebook: 'https://facebook.com/clarashih',
    github: undefined,
    website: 'https://www.salesforce.com/news/stories/clara-shih-salesforce-ai-ceo',
  },
  {
    id: '87',
    name: 'Marc Raibert',
    title: 'Executive Director, The AI Institute / Founder, Boston Dynamics',
    bio: 'Founder of Boston Dynamics, renowned for creating highly advanced and dynamic robots. Now Executive Director of The AI Institute.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Marc Raibert',
    expertise: ['Robotics', 'Dynamic Control Systems', 'AI Hardware', 'AI Leadership', 'Legged Locomotion'],
    impactArea: 'Key Innovators & Entrepreneurs',
    predictions: [],
    company: 'The AI Institute / Boston Dynamics',
    linkedin: 'https://linkedin.com/in/marc-raibert-3733a75',
    twitter: undefined,
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://theaiinstitute.com/leadership',
  },
  {
    id: '88',
    name: 'Timnit Gebru',
    title: 'Founder & Executive Director, DAIR Institute',
    bio: 'Founder of the Distributed AI Research (DAIR) Institute, a prominent voice in AI ethics, algorithmic bias, and fairness. Formerly at Google.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Timnit Gebru',
    expertise: ['AI Ethics', 'Algorithmic Bias', 'Fairness in AI', 'AI Policy', 'Accountability'],
    impactArea: 'Prominent Voices in AI Ethics & Safety',
    predictions: [],
    company: 'DAIR Institute (formerly Google)',
    linkedin: 'https://linkedin.com/in/timnit-gebru-5883831',
    twitter: 'https://twitter.com/timnitgebru',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://dair-institute.org',
  },
  {
    id: '89',
    name: 'Joy Buolamwini',
    title: 'Founder, Algorithmic Justice League',
    bio: 'Founder of the Algorithmic Justice League, known for exposing bias in facial recognition technology and advocating for equitable AI.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Joy Buolamwini',
    expertise: ['Algorithmic Bias', 'AI Ethics', 'Facial Recognition Technology', 'AI Policy', 'Social Justice'],
    impactArea: 'Prominent Voices in AI Ethics & Safety',
    predictions: [],
    company: 'Algorithmic Justice League',
    linkedin: 'https://linkedin.com/in/buolamwini',
    twitter: 'https://twitter.com/jovialjoy',
    instagram: 'https://instagram.com/jovialjoy',
    facebook: undefined,
    github: 'https://github.com/jovialjoy',
    website: 'https://poetofcode.com',
  },
  {
    id: '90',
    name: 'Kate Crawford',
    title: 'Research Professor, USC Annenberg / Sr. Principal Researcher, Microsoft Research',
    bio: 'Leading scholar on the social and political implications of AI, data systems, and artificial intelligence. Affiliated with USC Annenberg and Microsoft Research.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Kate Crawford',
    expertise: ['Social Implications of AI', 'AI Ethics', 'AI Policy', 'Data Studies', 'Power Dynamics'],
    impactArea: 'Prominent Voices in AI Ethics & Safety',
    predictions: [],
    company: 'USC Annenberg / Microsoft Research',
    linkedin: 'https://linkedin.com/in/kate-crawford-2b2ab622',
    twitter: 'https://twitter.com/katecrawford',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://www.katecrawford.net',
  },
  {
    id: '91',
    name: 'Oren Etzioni',
    title: 'Founding CEO Emeritus, Allen Institute for AI (AI2) / Professor, University of Washington',
    bio: 'Founding CEO Emeritus of AI2 and Professor at the University of Washington, a significant voice in AI ethics, impact, and commonsense reasoning.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Oren Etzioni',
    expertise: ['AI Ethics', 'AI Impact', 'Commonsense Reasoning', 'AI Leadership', 'Startups'],
    impactArea: 'Prominent Voices in AI Ethics & Safety',
    predictions: [],
    company: 'Allen Institute for AI (AI2) / University of Washington',
    linkedin: 'https://linkedin.com/in/oren-etzioni-ba8b2',
    twitter: 'https://twitter.com/etzioni',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://cs.washington.edu/people/faculty/etzioni',
  },
  {
    id: '92',
    name: 'Gary Marcus',
    title: 'Professor Emeritus, NYU / Founder, Robust.AI',
    bio: 'Professor Emeritus at NYU and founder of Robust.AI. A prominent critic of current deep learning approaches and advocate for neuro-symbolic AI.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Gary Marcus',
    expertise: ['Cognitive Science', 'Neuro-Symbolic AI', 'AI Criticism', 'AI Safety', 'Developmental Psychology'],
    impactArea: 'Prominent Voices in AI Ethics & Safety',
    predictions: [],
    company: 'NYU / Robust.AI',
    linkedin: 'https://linkedin.com/in/gary-marcus-3135881',
    twitter: 'https://twitter.com/garymarcus',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://garymarcus.com',
  },
  {
    id: '93',
    name: 'Francesca Rossi',
    title: 'IBM AI Ethics Global Leader',
    bio: 'IBM Fellow and AI Ethics Global Leader, working on embedding ethical principles into AI technologies and practices.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Francesca Rossi',
    expertise: ['AI Ethics', 'AI Governance', 'Responsible AI', 'AI Policy', 'Constraint Satisfaction'],
    impactArea: 'Prominent Voices in AI Ethics & Safety',
    predictions: [],
    company: 'IBM',
    linkedin: 'https://linkedin.com/in/francesca-rossi-7853491',
    twitter: 'https://twitter.com/frossi_t',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://research.ibm.com/people/francesca-rossi',
  },
  {
    id: '94',
    name: 'Rumman Chowdhury',
    title: 'CEO, Humane Intelligence',
    bio: 'CEO of Humane Intelligence and former Director of ML Ethics, Transparency & Accountability at Twitter, focusing on applied algorithmic ethics.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Rumman Chowdhury',
    expertise: ['Algorithmic Ethics', 'Responsible AI', 'AI Governance', 'AI Policy', 'Bias Mitigation'],
    impactArea: 'Prominent Voices in AI Ethics & Safety',
    predictions: [],
    company: 'Humane Intelligence (formerly Twitter)',
    linkedin: 'https://linkedin.com/in/rumman',
    twitter: 'https://twitter.com/ruchowdh',
    instagram: 'https://instagram.com/rumman.chowdhury',
    facebook: undefined,
    github: undefined,
    website: 'https://rummanchowdhury.com',
  },
  {
    id: '95',
    name: 'Eric Schmidt',
    title: 'Former CEO, Google / Chair, NSCAI',
    bio: 'Former CEO of Google and Chair of the National Security Commission on Artificial Intelligence (NSCAI), influential in AI policy and national security discussions.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Eric Schmidt',
    expertise: ['AI Policy', 'National Security', 'Technology Leadership', 'AI Strategy', 'Government'],
    impactArea: 'Prominent Voices in AI Ethics & Safety',
    predictions: [],
    company: 'Formerly Google, NSCAI',
    linkedin: 'https://linkedin.com/in/ericschmidt',
    twitter: 'https://twitter.com/ericschmidt',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://ericschmidt.com',
  },
  {
    id: '96',
    name: 'James Manyika',
    title: 'SVP, Research, Technology & Society, Google',
    bio: 'Senior Vice President at Google, focusing on the economic and societal impact of technology, including AI.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'James Manyika',
    expertise: ['Societal Impact of AI', 'AI Economics', 'AI Policy', 'Technology Research', 'Future of Work'],
    impactArea: 'Prominent Voices in AI Ethics & Safety',
    predictions: [],
    company: 'Google',
    linkedin: 'https://linkedin.com/in/james-manyika-a802425',
    twitter: 'https://twitter.com/jamesmanyika',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://about.google/intl/en/our-story/leaders/james-manyika',
  },
  {
    id: '97',
    name: 'DJ Patil',
    title: 'Executive, Devoted Health',
    bio: 'Former U.S. Chief Data Scientist, a leading voice in data ethics, AI for good, and applying data science to solve societal challenges. Now at Devoted Health.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'DJ Patil',
    expertise: ['Data Ethics', 'AI for Good', 'Data Science', 'AI Policy', 'Public Policy'],
    impactArea: 'Prominent Voices in AI Ethics & Safety',
    predictions: [],
    company: 'Devoted Health (formerly U.S. Government)',
    linkedin: 'https://linkedin.com/in/dpatil',
    twitter: 'https://twitter.com/dpatil',
    instagram: undefined,
    facebook: undefined,
    github: 'https://github.com/dpatil',
    website: 'https://djp.li',
  },
  {
    id: '98',
    name: 'Helen Toner',
    title: 'Director of Strategy, Center for Security and Emerging Technology (CSET)',
    bio: 'Director of Strategy at CSET and former OpenAI board member, focusing on AI policy, safety, and US-China tech competition.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Helen Toner',
    expertise: ['AI Policy', 'AI Safety', 'Technology Governance', 'International Security', 'China AI'],
    impactArea: 'Prominent Voices in AI Ethics & Safety',
    predictions: [],
    company: 'Center for Security and Emerging Technology (CSET) (formerly OpenAI board)',
    linkedin: 'https://linkedin.com/in/helen-toner',
    twitter: 'https://twitter.com/helentoner',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://cset.georgetown.edu/people/helen-toner',
  },
  {
    id: '99',
    name: 'Eliezer Yudkowsky',
    title: 'Co-founder & Research Fellow, Machine Intelligence Research Institute (MIRI)',
    bio: 'Co-founder of MIRI, a prominent researcher and writer on AI alignment, superintelligence, and existential risk from advanced AI.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Eliezer Yudkowsky',
    expertise: ['AI Safety', 'Existential Risk', 'Superintelligence', 'Rationality', 'Decision Theory'],
    impactArea: 'Prominent Voices in AI Ethics & Safety',
    predictions: [],
    company: 'Machine Intelligence Research Institute (MIRI)',
    linkedin: undefined,
    twitter: 'https://twitter.com/esryudkowsky',
    instagram: undefined,
    facebook: 'https://facebook.com/eliezer.yudkowsky',
    github: undefined,
    website: 'https://yudkowsky.net',
  },
  {
    id: '100',
    name: 'Nick Bostrom',
    title: 'Professor, University of Oxford / Director, Future of Humanity Institute',
    bio: 'Professor at Oxford University and Director of the Future of Humanity Institute, author of "Superintelligence," focusing on AI safety and existential risk.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Nick Bostrom',
    expertise: ['AI Safety', 'Existential Risk', 'Superintelligence', 'Philosophy of AI', 'Transhumanism'],
    impactArea: 'Prominent Voices in AI Ethics & Safety',
    predictions: [],
    company: 'University of Oxford / Future of Humanity Institute',
    linkedin: undefined,
    twitter: undefined,
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://nickbostrom.com',
  },
  {
    id: '101',
    name: 'Max Tegmark',
    title: 'Professor, MIT / President, Future of Life Institute',
    bio: 'Professor at MIT and President of the Future of Life Institute, a physicist and AI safety advocate, author of "Life 3.0."',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Max Tegmark',
    expertise: ['AI Safety', 'Physics', 'Existential Risk', 'Future of AI', 'Consciousness'],
    impactArea: 'Prominent Voices in AI Ethics & Safety',
    predictions: [],
    company: 'MIT / Future of Life Institute',
    linkedin: 'https://linkedin.com/in/max-tegmark-5ba4313',
    twitter: 'https://twitter.com/tegmark',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://space.mit.edu/home/tegmark',
  },
  {
    id: '102',
    name: 'Noam Brown',
    title: 'Research Scientist, Meta AI',
    bio: 'Research Scientist at Meta AI, known for developing AI that achieved superhuman performance in complex games like Poker (Pluribus) and Diplomacy (Cicero).',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Noam Brown',
    expertise: ['Game AI', 'Reinforcement Learning', 'Multi-Agent Systems', 'Deep Learning', 'Computational Game Theory'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [],
    company: 'Meta AI',
    linkedin: 'https://linkedin.com/in/noam-brown-64150334',
    twitter: 'https://twitter.com/noambrown',
    instagram: undefined,
    facebook: undefined,
    github: 'https://github.com/noambrown',
    website: 'https://www.cs.cmu.edu/~noamb',
  },
  {
    id: '103',
    name: 'Armand Joulin',
    title: 'Research Scientist, Meta AI',
    bio: 'Research Scientist at Meta AI, contributing to efficient NLP models (e.g., fastText) and computer vision.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Armand Joulin',
    expertise: ['Natural Language Processing', 'Computer Vision', 'Efficient AI Models', 'Machine Learning', 'fastText'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [],
    company: 'Meta AI',
    linkedin: 'https://linkedin.com/in/armand-joulin-91a74212',
    twitter: 'https://twitter.com/armandjoulin',
    instagram: undefined,
    facebook: undefined,
    github: 'https://github.com/facebookresearch/fastText',
    website: 'https://research.facebook.com/people/joulin-armand',
  },
  {
    id: '104',
    name: 'Volodymyr Mnih',
    title: 'Research Scientist, Google DeepMind',
    bio: 'Research Scientist at Google DeepMind, a key figure in developing Deep Q-Networks (DQN), a breakthrough in deep reinforcement learning.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Volodymyr Mnih',
    expertise: ['Reinforcement Learning', 'Deep Learning', 'Neural Networks', 'Game AI', 'Deep Q-Networks (DQN)'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [],
    company: 'Google DeepMind',
    linkedin: 'https://linkedin.com/in/volodymyr-mnih-6bba433',
    twitter: 'https://twitter.com/vmnih',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://www.cs.toronto.edu/~vmnih',
  },
  {
    id: '105',
    name: 'Larry Heck',
    title: 'Director, AI, Google',
    bio: 'Director of AI at Google, with extensive experience in speech recognition and conversational AI. Formerly at Samsung and Microsoft.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Larry Heck',
    expertise: ['Speech Recognition', 'Conversational AI', 'Natural Language Processing', 'AI Research', 'Deep Learning'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [],
    company: 'Google (formerly Samsung, Microsoft)',
    linkedin: 'https://linkedin.com/in/larry-heck-8a03892',
    twitter: 'https://twitter.com/lheck',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://ece.gatech.edu/faculty-staff-directory/larry-p-heck',
  },
  {
    id: '106',
    name: 'Li Deng',
    title: 'Chief AI Officer, Citadel',
    bio: 'Chief AI Officer at Citadel. Formerly Chief AI Officer at Microsoft and Head of AI Research at DeepMind, pioneering deep learning for speech recognition and finance.',
    avatarUrl: 'https://placehold.co/400x400.png',
    dataAiHint: 'Li Deng',
    expertise: ['Deep Learning', 'Speech Recognition', 'Financial AI', 'AI Strategy', 'Natural Language Processing'],
    impactArea: 'Leaders at Major AI Labs & Companies',
    predictions: [],
    company: 'Citadel (formerly Microsoft, DeepMind)',
    linkedin: 'https://linkedin.com/in/li-deng-48a1061',
    twitter: 'https://twitter.com/dr_li_deng',
    instagram: undefined,
    facebook: undefined,
    github: undefined,
    website: 'https://lideng.com',
  }
];

export const getAllExpertise = (): string[] => {
  const allExpertise = new Set<string>();
  experts.forEach(expert => {
    expert.expertise.forEach(e => allExpertise.add(e));
  });
  return Array.from(allExpertise).sort();
};

export const getAllImpactAreas = (): string[] => {
  return IMPACT_AREAS_ORDERED;
};




